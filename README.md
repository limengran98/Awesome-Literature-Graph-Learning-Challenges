
# ðŸ“š Awesome Literature: Graph Learning Challenges with LLMs

A curated list of recent research addressing fundamental challenges in graph learning with the assistance of large language models (LLMs). Papers are categorized by **challenge type** and **methodological approach**.

<p align="center">
  <img src="https://github.com/limengran98/Awesome-Literature-Graph-Learning-Challenges/blob/main/fig.jpg" width="80%">
</p>


*The four fundamental challenges emerge of real-world graph complexity: (1) **Incompleteness** in graphs, where nodes, edges, or attributes are missing, (2) **Imbalance** in graphs, where the distribution of nodes, edges, or labels is highly skewed, (3) **Cross-domain heterogeneity** in graphs, where graph data from different domains exhibit semantic and structural discrepancies, and (4) **Dynamic instability** in graphs, where graphs undergo dynamic changes in topology, attributes, or interactions over time.*

## ðŸ“° News
- **2025-09-15**: Updated the **datasets, metrics, and tasks** sections with new entries and anchors.
- **2025-09-08**: Our survey paper was accepted by [*Expert Systems with Applications (ESWA)* ðŸŽ‰](https://www.sciencedirect.com/science/article/abs/pii/S0957417425032580)  

## Table of Contents

- [Incompleteness in Graphs](#incompleteness-in-graphs)
  - [Robust Graph Learning](#robust-graph-learning)
  - [Few-shot Graph Learning](#few-shot-graph-learning)
  - [Knowledge Graph Completion](#knowledge-graph-completion)
  - [Datasets, Metrics, and Tasks (Incompleteness)](#datasets-metrics-and-tasks-incompleteness)
- [Imbalance in Graphs](#imbalance-in-graphs)
  - [Class-Imbalanced Graph Learning](#class-imbalanced-graph-learning)
  - [Structure-Imbalanced Graph Learning](#structure-imbalanced-graph-learning)
  - [Datasets, Metrics, and Tasks (Imbalance)](#datasets-metrics-and-tasks-imbalance)
- [Cross-Domain Heterogeneity in Graphs](#cross-domain-heterogeneity-in-graphs)
  - [Text-Attributed Graph Learning](#text-attributed-graph-learning)
  - [Multimodal Attributed Graph Learning](#multimodal-attributed-graph-learning)
  - [Structural Heterogeneous Graph Learning](#structural-heterogeneous-graph-learning)
  - [Datasets, Metrics, and Tasks (Cross-domain Heterogeneity)](#datasets-metrics-and-tasks-cross-domain-heterogeneity)
- [Dynamic Instability in Graphs](#dynamic-instability-in-graphs)
  - [LLMs for Querying and Reasoning](#querying-and-reasoning)
  - [LLMs for Generating and Updating](#generating-and-updating)
  - [LLMs for Evaluation and Application](#evaluation-and-application)
  - [Datasets, Metrics, and Tasks (Dynamic Instability)](#datasets-metrics-and-tasks-dynamic-instability)

---

##  Incompleteness in Graphs

> Graphs often suffer from missing node features, incomplete edges, or absent labels. These works tackle incompleteness via robust training, knowledge augmentation, or few-shot reasoning.

###  Robust Graph Learning
<!--
| Title | Authors | Venue & Year | Link |
|-------|---------|---------------|------|
| Spatiotemporal Pre-Trained Large Language Model for Forecasting With Missing Values | Le Fang, Wei Xiang, Shirui Pan, Flora D. Salim, Yi-Ping Phoebe Chen | IEEE Internet of Things Journal, 2025 | [Link](https://doi.org/10.1109/JIOT.2024.3524030) |
| LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs | Kai Wang, Yuwei Xu, Zhiyong Wu, Siqiang Luo | Findings of the Association for Computational Linguistics, 2024 | [Link](https://doi.org/10.18653/v1/2024.findings-acl.224) |
| On LLM-Enhanced Mixed-Type Data Imputation with High-Order Message Passing | Jianwei Wang, Kai Wang, Ying Zhang, Wenjie Zhang, Xiwei Xu, Xuemin Lin | arXiv preprint, 2025 | [Link](https://arxiv.org/abs/2501.02191) |
| Large language models as topological structure enhancers for text-attributed graphs | Shengyin Sun, Yuxiang Ren, Chen Ma, Xuecang Zhang | arXiv preprint, 2023 | [Link](https://doi.org/10.48550/arXiv.2311.14324) |
| Empower text-attributed graphs learning with large language models (LLMs) | Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang Li, Xuecang Zhang | arXiv preprint, 2023 | [Link](https://doi.org/10.48550/arXiv.2310.09872) |
| Label-free node classification on graphs with large language models (LLMs) | Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang | arXiv preprint, 2023 | [Link](https://arxiv.org/abs/2310.04668) |
| GraphLLM: Boosting graph reasoning ability of large language models | Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, Yang Yang | arXiv preprint, 2023 | [Link](https://doi.org/10.48550/arXiv.2310.05845) |
-->

<details>
<summary><u><strong> Spatiotemporal Pre-Trained Large Language Model for Forecasting With Missing Values (2025)</strong></u></summary>

**Authors:** Le Fang, Wei Xiang, Shirui Pan, Flora D. Salim, Yi-Ping Phoebe Chen  
**Venue & Year:** IEEE Internet of Things Journal, 2025  
**Link:** [https://doi.org/10.1109/JIOT.2024.3524030](https://doi.org/10.1109/JIOT.2024.3524030)  
**Abstract:**  
Spatiotemporal data collected by sensors within an urban Internet of Things (IoT) system inevitably contains some missing values, which significantly affects the accuracy of spatiotemporal data forecasting. However, existing techniques, including those based on large language models (LLMs), show limited effectiveness in forecasting with missing values, especially in scenarios involving high-dimensional sensor data. In this article, we propose a novel spatiotemporal pretrained LLM dubbed SPLLM for forecasting with missing values. In this network, we seamlessly integrate a specialized spatiotemporal fusion graph convolutional network (GCN) module that extracts intricate spatiotemporal and graph-based information, for generating suitable inputs to the SPLLM. Furthermore, we propose a feed-forward network (FFN) fine-tuning strategy within the LLM and a final fusion layer to enable the model to leverage the pretrained foundational knowledge of the LLM and adapt to new incomplete data simultaneously. The experimental results indicate that SPLLM outperforms state-of-the-art models on real-world public datasets. Notably, SPLLM exhibits a superior performance in tackling incomplete sensory data with a variety of missing rates. A comprehensive ablation study of key components is conducted to demonstrate their efficiency.
</details>
<details>
<summary><u><strong> LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs (2024)</strong></u></summary>

**Authors:** Kai Wang, Yuwei Xu, Zhiyong Wu, Siqiang Luo  
**Venue & Year:** Findings of the Association for Computational Linguistics, 2024  
**Link:** [https://doi.org/10.18653/v1/2024.findings-acl.224](https://doi.org/10.18653/v1/2024.findings-acl.224)  
**Abstract:**  
Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-resource KG datasets and find that ProLINK outperforms previous methods in three-shot, one-shot, and zero-shot reasoning tasks, exhibiting average performance improvements by 20%, 45%, and 147%, respectively. Furthermore, ProLINK demonstrates strong robustness for various LLM promptings as well as full-shot scenarios.

</details>
<details>
<summary><u><strong> On LLM-Enhanced Mixed-Type Data Imputation with High-Order Message Passing (2025)</strong></u></summary>

**Authors:** Jianwei Wang, Kai Wang, Ying Zhang, Wenjie Zhang, Xiwei Xu, Xuemin Lin  
**Venue & Year:** arXiv preprint, 2025  
**Link:** [https://arxiv.org/abs/2501.02191](https://arxiv.org/abs/2501.02191)  
**Abstract:**  
Missing data imputation, which aims to impute the missing values in the raw datasets to achieve the completeness of datasets, is crucial for modern data-driven models like large language models (LLMs) and has attracted increasing interest over the past decades. Despite its importance, existing solutions for missing data imputation either 1) only support numerical and categorical data or 2) show an unsatisfactory performance due to their design prioritizing text data and the lack of key properties for tabular data imputation. In this paper, we propose UnIMP, a Unified IMPutation framework that leverages LLM and high-order message passing to enhance the imputation of mixed-type data including numerical, categorical, and text data. Specifically, we first introduce a cell-oriented hypergraph to model the table. We then propose BiHMP, an efficient Bidirectional High-order Message-Passing network to aggregate global-local information and high-order relationships on the constructed hypergraph while capturing the inter-column heterogeneity and intra-column homogeneity. To effectively and efficiently align the capacity of the LLM with the information aggregated by BiHMP, we introduce Xfusion, which, together with BiHMP, acts as adapters for the LLM. We follow a pre-training and fine-tuning pipeline to train UnIMP, integrating two optimizations: chunking technique, which divides tables into smaller chunks to enhance efficiency; and progressive masking technique, which gradually adapts the model to learn more complex data patterns. Both theoretical proofs and empirical experiments on 10 real world datasets highlight the superiority of UnIMP over existing techniques.
</details>
<details>
<summary><u><strong> Large language models as topological structure enhancers for text-attributed graphs (2023)</strong></u></summary>

**Authors:** Shengyin Sun, Yuxiang Ren, Chen Ma, Xuecang Zhang  
**Venue & Year:** arXiv preprint, 2023  
**Link:** [https://doi.org/10.48550/arXiv.2311.14324](https://doi.org/10.48550/arXiv.2311.14324)  
**Abstract:**  
The latest advancements in large language models (LLMs) have revolutionized the field of natural language processing (NLP). Inspired by the success of LLMs in NLP tasks, some recent work has begun investigating the potential of applying LLMs in graph learning tasks. However, most of the existing work focuses on utilizing LLMs as powerful node feature augmenters, leaving employing LLMs to enhance graph topological structures an understudied problem. In this work, we explore how to leverage the information retrieval and text generation capabilities of LLMs to refine/enhance the topological structure of text-attributed graphs (TAGs) under the node classification setting. First, we propose using LLMs to help remove unreliable edges and add reliable ones in the TAG. Specifically, we first let the LLM output the semantic similarity between node attributes through delicate prompt designs, and then perform edge deletion and edge addition based on the similarity. Second, we propose using pseudo-labels generated by the LLM to improve graph topology, that is, we introduce the pseudo-label propagation as a regularization to guide the graph neural network (GNN) in learning proper edge weights. Finally, we incorporate the two aforementioned LLM-based methods for graph topological refinement into the process of GNN training, and perform extensive experiments on four real-world datasets. The experimental results demonstrate the effectiveness of LLM-based graph topology refinement (achieving a 0.15%--2.47% performance gain on public benchmarks).
</details>
<details>
<summary><u><strong> Leveraging Large Language Models for Node Generation in Few-Shot Learning on Text-Attributed Graphs (2025)</strong></u></summary>

**Authors:** Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang Li, Xuecang Zhang  
**Venue & Year:** AAAI, 2025  
**Link:** [https://doi.org/10.48550/arXiv.2310.09872](https://doi.org/10.48550/arXiv.2310.09872)  
**Abstract:**  
Text-attributed graphs have recently garnered significant attention due to their wide range of applications in web domains. Existing methodologies employ word embedding models for acquiring text representations as node features, which are subsequently fed into Graph Neural Networks (GNNs) for training. Recently, the advent of Large Language Models (LLMs) has introduced their powerful capabilities in information retrieval and text generation, which can greatly enhance the text attributes of graph data. Furthermore, the acquisition and labeling of extensive datasets are both costly and time-consuming endeavors. Consequently, few-shot learning has emerged as a crucial problem in the context of graph learning tasks. In order to tackle this challenge, we propose a lightweight paradigm called LLM4NG, which adopts a plug-and-play approach to empower text-attributed graphs through node generation using LLMs. Specifically, we utilize LLMs to extract semantic information from the labels and generate samples that belong to these categories as exemplars. Subsequently, we employ an edge predictor to capture the structural information inherent in the raw dataset and integrate the newly generated samples into the original graph. This approach harnesses LLMs for enhancing class-level information and seamlessly introduces labeled nodes and edges without modifying the raw dataset, thereby facilitating the node classification task in few-shot scenarios. Extensive experiments demonstrate the outstanding performance of our proposed paradigm, particularly in low-shot scenarios. For instance, in the 1-shot setting of the ogbn-arxiv dataset, LLM4NG achieves a 76% improvement over the baseline model.
</details>
<details>
<summary><u><strong> Label-free node classification on graphs with large language models (LLMs) (2023)</strong></u></summary>

**Authors:** Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang  
**Venue & Year:** arXiv preprint, 2023  
**Link:** [https://arxiv.org/abs/2310.04668](https://arxiv.org/abs/2310.04668)  
**Abstract:**  
In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to obtain annotations of high quality, representativeness, and diversity, thereby enhancing GNN performance with less cost? To tackle this challenge, we develop an annotation quality heuristic and leverage the confidence scores derived from LLMs to advanced node selection. Comprehensive experimental results validate the effectiveness of LLM-GNN. In particular, LLM-GNN can achieve an accuracy of 74.9% on a vast-scale dataset \products with a cost less than 1 dollar.
</details>
<details>
<summary><u><strong> GraphLLM: Boosting graph reasoning ability of large language models (2023)</strong></u></summary>

**Authors:** Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, Yang Yang  
**Venue & Year:** arXiv preprint, 2023  
**Link:** [https://doi.org/10.48550/arXiv.2310.05845](https://doi.org/10.48550/arXiv.2310.05845)  
**Abstract:**  
The advancement of Large Language Models (LLMs) has remarkably pushed the boundaries towards artificial general intelligence (AGI), with their exceptional ability on understanding diverse types of information, including but not limited to images and audio. Despite this progress, a critical gap remains in empowering LLMs to proficiently understand and reason on graph data. Recent studies underscore LLMs' underwhelming performance on fundamental graph reasoning tasks. In this paper, we endeavor to unearth the obstacles that impede LLMs in graph reasoning, pinpointing the common practice of converting graphs into natural language descriptions (Graph2Text) as a fundamental bottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering end-to-end approach that synergistically integrates graph learning models with LLMs. This synergy equips LLMs with the ability to proficiently interpret and reason on graph data, harnessing the superior expressive power of graph learning models. Our empirical evaluations across four fundamental graph reasoning tasks validate the effectiveness of GraphLLM. The results exhibit a substantial average accuracy enhancement of 54.44%, alongside a noteworthy context reduction of 96.45% across various graph reasoning tasks.
</details>


---

###  Few-shot Graph Learning
<!--
| Title | Authors | Venue & Year | Link |
|-------|---------|---------------|------|
| LLM-Empowered Few-Shot Node Classification on Incomplete Graphs with Real Node Degrees | Yun Li, Yi Yang, Jiaqi Zhu, Hui Chen, Hongan Wang | Proceedings of the ACM International Conference on Information and Knowledge Management, 2024 | [Link](https://doi.org/10.1145/3627673.3679861) |
| LinkGPT: Teaching Large Language Models to Predict Missing Links | Zhongmou He, Jing Zhu, Shengyi Qian, Joyce Chai, Danai Koutra | arXiv preprint, 2024 | [Link](https://doi.org/10.48550/arXiv.2406.04640) |
| AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using Large Language Models | Shuo Liu, Di Yao, Lanting Fang, Zhetao Li, Wenbin Li, Kaiyu Feng, XiaoWen Ji, Jingping Bi | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2405.07626) |
| HeGTa: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding | Rihui Jin, Yu Li, Guilin Qi, Nan Hu, Yuan-Fang Li, Jiaoyan Chen, Jianan Wang, Yongrui Chen, Dehai Min, Sheng Bi | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2403.19723) |
| FlexKBQA: A Flexible LLM-powered Framework for Few-shot Knowledge Base Question Answering | Zhenyu Li, Sunqi Fan, Yu Gu, Xiuxing Li, Zhichao Duan, Bowen Dong, Ning Liu, Jianyong Wang | Proceedings of the AAAI Conference on Artificial Intelligence, 2024 | [Link](https://doi.org/10.1609/aaai.v38i17.29823) |
| Zero-shot Knowledge Graph Question Generation via Multi-agent LLMs and Small Models Synthesis | Runhao Zhao, Jiuyang Tang, Weixin Zeng, Ziyang Chen, Xiang Zhao | Proceedings of the ACM International Conference on Information and Knowledge Management, 2024 | [Link](https://doi.org/10.1145/3627673.3679805) |
-->
<details>
<summary><u><strong>LLM-Empowered Few-Shot Node Classification on Incomplete Graphs with Real Node Degrees (2024)</strong></u></summary>

**Authors:** Yun Li, Yi Yang, Jiaqi Zhu, Hui Chen, Hongan Wang  
**Venue & Year:** Proceedings of the ACM International Conference on Information and Knowledge Management, 2024  
**Link:** [https://doi.org/10.1145/3627673.3679861](https://doi.org/10.1145/3627673.3679861)  
**Abstract:**  
Knowledge Graph Question Generation (KGQG) is the task of generating natural language questions based on the given knowledge graph (KG). Although extensively explored in recent years, prevailing models predominantly depend on labelled data for training deep learning models or employ large parametric frameworks, e.g., Large Language Models (LLMs), which can incur significant deployment costs and pose practical implementation challenges. To address these issues, in this work, we put forward a zero-shot, multi-agent KGQG framework. This framework integrates the capabilities of LLMs with small models to facilitate cost-effective, high-quality question generation. In specific, we develop a professional editorial team architecture accompanied by two workflow optimization tools to reduce unproductive collaboration among LLMs-based agents and enhance the robustness of the system. Extensive experiments demonstrate that our proposed framework derives the new state-of-the-art performance on the zero-shot KGQG tasks, with relative gains of 20.24% and 13.57% on two KGQG datasets, respectively, which rival fully supervised state-of-the-art models.
</details>
<details>
<summary><u><strong>LinkGPT: Teaching Large Language Models to Predict Missing Links (2024)</strong></u></summary>

**Authors:** Zhongmou He, Jing Zhu, Shengyi Qian, Joyce Chai, Danai Koutra  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://doi.org/10.48550/arXiv.2406.04640](https://doi.org/10.48550/arXiv.2406.04640)  
**Abstract:**  
</details>
<details>
<summary><u><strong>AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using Large Language Models (2024)</strong></u></summary>

**Authors:** Shuo Liu, Di Yao, Lanting Fang, Zhetao Li, Wenbin Li, Kaiyu Feng, XiaoWen Ji, Jingping Bi  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2405.07626](https://arxiv.org/abs/2405.07626)  
**Abstract:**  
Large Language Models (LLMs) have shown promising results on various language and vision tasks. Recently, there has been growing interest in applying LLMs to graph-based tasks, particularly on Text-Attributed Graphs (TAGs). However, most studies have focused on node classification, while the use of LLMs for link prediction (LP) remains understudied. In this work, we propose a new task on LLMs, where the objective is to leverage LLMs to predict missing links between nodes in a graph. This task evaluates an LLM's ability to reason over structured data and infer new facts based on learned patterns. This new task poses two key challenges: (1) How to effectively integrate pairwise structural information into the LLMs, which is known to be crucial for LP performance, and (2) how to solve the computational bottleneck when teaching LLMs to perform LP. To address these challenges, we propose LinkGPT, the first end-to-end trained LLM for LP tasks. To effectively enhance the LLM's ability to understand the underlying structure, we design a two-stage instruction tuning approach where the first stage fine-tunes the pairwise encoder, projector, and node projector, and the second stage further fine-tunes the LLMs to predict links. To address the efficiency challenges at inference time, we introduce a retrieval-reranking scheme. Experiments show that LinkGPT can achieve state-of-the-art performance on real-world graphs as well as superior generalization in zero-shot and few-shot learning, surpassing existing benchmarks. At inference time, it can achieve 10\times speedup while maintaining high LP accuracy.
</details>
<details>
<summary><u><strong>HeGTa: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding (2024)</strong></u></summary>

**Authors:** Rihui Jin, Yu Li, Guilin Qi, Nan Hu, Yuan-Fang Li, Jiaoyan Chen, Jianan Wang, Yongrui Chen, Dehai Min, Sheng Bi  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2403.19723](https://arxiv.org/abs/2403.19723)  
**Abstract:**  
Table understanding (TU) has achieved promising advancements, but it faces the challenges of the scarcity of manually labeled tables and the presence of complex table this http URL address these challenges, we propose HGT, a framework with a heterogeneous graph (HG)-enhanced large language model (LLM) to tackle few-shot TU this http URL leverages the LLM by aligning the table semantics with the LLM's parametric knowledge through soft prompts and instruction turning and deals with complex tables by a multi-task pre-training scheme involving three novel multi-granularity self-supervised HG pre-training this http URL empirically demonstrate the effectiveness of HGT, showing that it outperforms the SOTA for few-shot complex TU on several benchmarks.
</details>
<details>
<summary><u><strong>FlexKBQA: A Flexible LLM-powered Framework for Few-shot Knowledge Base Question Answering (2024)</strong></u></summary>

**Authors:** Zhenyu Li, Sunqi Fan, Yu Gu, Xiuxing Li, Zhichao Duan, Bowen Dong, Ning Liu, Jianyong Wang  
**Venue & Year:** Proceedings of the AAAI Conference on Artificial Intelligence, 2024  
**Link:** [https://doi.org/10.1609/aaai.v38i17.29823](https://doi.org/10.1609/aaai.v38i17.29823)  
**Abstract:**  
Knowledge base question answering (KBQA) is a critical yet challenging task due to the vast number of entities within knowledge bases and the diversity of natural language questions posed by users. Unfortunately, the performance of most KBQA models tends to decline significantly in real-world scenarios where high-quality annotated data is insufficient. To mitigate the burden associated with manual annotation, we introduce FlexKBQA by utilizing Large Language Models (LLMs) as program translators for addressing the challenges inherent in the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms to sample diverse programs, such as SPARQL queries, from the knowledge base, which are subsequently converted into natural language questions via LLMs. This synthetic dataset facilitates training a specialized lightweight model for the KB. Additionally, to reduce the barriers of distribution shift between synthetic data and real user questions, FlexKBQA introduces an executionguided self-training method to iterative leverage unlabeled user questions. Furthermore, we explore harnessing the inherent reasoning capability of LLMs to enhance the entire framework. Consequently, FlexKBQA delivers substantial flexibility, encompassing data annotation, deployment, and being domain agnostic. Through extensive experiments on GrailQA, WebQSP, and KQA Pro, we observe that under the few-shot even the more challenging zero-shot scenarios, FlexKBQA achieves impressive results with a few annotations, surpassing all previous baselines and even approaching the performance of supervised models, achieving a remarkable 93% performance relative to the fully-supervised models. We posit that FlexKBQA represents a significant advancement towards exploring better integration of large and lightweight models. Code is available at https://github.com/leezythu/FlexKBQA.
</details>
<details>
<summary><u><strong>Zero-shot Knowledge Graph Question Generation via Multi-agent LLMs and Small Models Synthesis (2024)</strong></u></summary>

**Authors:** Runhao Zhao, Jiuyang Tang, Weixin Zeng, Ziyang Chen, Xiang Zhao  
**Venue & Year:** Proceedings of the ACM International Conference on Information and Knowledge Management, 2024  
**Link:** [https://doi.org/10.1145/3627673.3679805](https://doi.org/10.1145/3627673.3679805)  
**Abstract:**  
Knowledge Graph Question Generation (KGQG) is the task of generating natural language questions based on the given knowledge graph (KG). Although extensively explored in recent years, prevailing models predominantly depend on labelled data for training deep learning models or employ large parametric frameworks, e.g., Large Language Models (LLMs), which can incur significant deployment costs and pose practical implementation challenges. To address these issues, in this work, we put forward a zero-shot, multi-agent KGQG framework. This framework integrates the capabilities of LLMs with small models to facilitate cost-effective, high-quality question generation. In specific, we develop a professional editorial team architecture accompanied by two workflow optimization tools to reduce unproductive collaboration among LLMs-based agents and enhance the robustness of the system. Extensive experiments demonstrate that our proposed framework derives the new state-of-the-art performance on the zero-shot KGQG tasks, with relative gains of 20.24% and 13.57% on two KGQG datasets, respectively, which rival fully supervised state-of-the-art models.
</details>

---

###  Knowledge Graph Completion
<!--
| Title | Authors | Venue & Year | Link |
|-------|---------|---------------|------|
| GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language for Knowledge Graph Completion | Kangyang Luo, Yuzhuo Bai, Cheng Gao, Shuzheng Si, Yingli Shen, Zhu Liu, Zhitong Wang, Wenhao Li, Yufei Huang | arXiv preprint, 2025 | [Link](https://www.arxiv.org/abs/2502.11471) |
| GS-KGC: A generative subgraph-based framework for knowledge graph completion with large language models | Rui Yang, Jiahao Zhu, Jianping Man, Hongze Liu, Li Fang, Yi Zhou | Information Fusion, 2025 | [Link](https://doi.org/10.1016/j.inffus.2024.102868) |
| In-Context Learning with Topological Information for LLM-Based Knowledge Graph Completion | Udari Madhushani Sehwag, Kassiani Papasotiriou, Jared Vann, Sumitra Ganesh | ICML 2024 Workshop on Structured Probabilistic Inference & Generative Modeling | [Link](https://openreview.net/forum?id=eUpH8AuVQa) |
| Making large language models perform better in knowledge graph completion | Yichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu, Wen Zhang, Hlmruajun Chen | Proceedings of the 32nd ACM International Conference on Multimedia, 2024 | [Link](https://doi.org/10.1145/3664647.3681327) |
| LLM-based multi-level knowledge generation for few-shot knowledge graph completion | Qian Li, Zhuo Chen, Cheng Ji, Shiqi Jiang, Jianxin Li | Proceedings of the International Joint Conference on Artificial Intelligence, 2024 | [Link](https://www.ijcai.org/proceedings/2024/236) |
| Assessing LLMs Suitability for Knowledge Graph Completion | Vasile Ionut Remus Iga, Gheorghe Cosmin Silaghi | International Conference on Neural-Symbolic Learning and Reasoning, 2024 | [Link](https://doi.org/10.1007/978-3-031-71170-1_22) |
| Finetuning generative large language models with discrimination instructions for knowledge graph completion | Yang Liu, Xiaobin Tian, Zequn Sun, Wei Hu | International Semantic Web Conference, 2024 | [Link](https://doi.org/10.1007/978-3-031-77844-5_11) |
| Enhancing text-based knowledge graph completion with zero-shot large language models: A focus on semantic enhancement | Rui Yang, Jiahao Zhu, Jianping Man, Li Fang, Yi Zhou | Knowledge-Based Systems, 2024 | [Link](https://doi.org/10.1016/j.knosys.2024.112155) |
| Framing Few-Shot Knowledge Graph Completion with Large Language Models | Adrian MP Brasoveanu, Lyndon Nixon, Albert Weichselbraun, Arno Scharl | Joint Workshop Proceedings of the 5th International Workshop on Sem4Tra and SEMANTiCS, 2023 | [Link](https://ceur-ws.org/Vol-3510/paper_nlp_4.pdf) |
| Iterative zero-shot LLM prompting for knowledge graph construction | Salvatore Carta, Alessandro Giuliani, Leonardo Piano, Alessandro Sebastian Podda, Livio Pompianu, Sandro Gabriele Tiddia | arXiv preprint, 2023 | [Link](https://doi.org/10.48550/ARXIV.2307.01128) |
| Exploring large language models for knowledge graph completion | Liang Yao, Jiazhen Peng, Chengsheng Mao, Yuan Luo | arXiv preprint, 2023 | [Link](https://doi.org/10.48550/arXiv.2308.13916) |
| KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion | Yanbin Wei, Qiushi Huang, Yu Zhang, James Kwok | Findings of the Association for Computational Linguistics: EMNLP, 2023 | [Link](https://doi.org/10.18653/v1/2023.findings-emnlp.580) |
| Knowledge graph completion models are few-shot learners: An empirical study of relation labeling in e-commerce with LLMs | Jiao Chen, Luyi Ma, Xiaohan Li, Nikhil Thakurdesai, Jianpeng Xu, Jason HD Cho, Kaushiki Nag, Evren Korpeoglu, Sushant Kumar, Kannan Achan | arXiv preprint, 2023 | [Link](https://doi.org/10.48550/arXiv.2305.09858) |
| Generate-on-graph: Treat LLM as both agent and KG in incomplete knowledge graph question answering | Yao Xu, Shizhu He, Jiabei Chen, Zihao Wang, Yangqiu Song, Hanghang Tong, Guang Liu, Kang Liu, Jun Zhao | arXiv preprint, 2024 | [Link](https://doi.org/10.48550/arXiv.2307.01128) |
-->

<details>
<summary><u><strong>GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language for Knowledge Graph Completion (2025)</strong></u></summary>

**Authors:** Kangyang Luo, Yuzhuo Bai, Cheng Gao, Shuzheng Si, Yingli Shen, Zhu Liu, Zhitong Wang, Wenhao Li, Yufei Huang  
**Venue & Year:** arXiv preprint, 2025  
**Link:** [https://www.arxiv.org/abs/2502.11471](https://www.arxiv.org/abs/2502.11471)  
**Abstract:**  
Multiple-input multiple-output (MIMO) radar offers several performance and flexibility advantages over traditional radar arrays. However, high angular and Doppler resolutions necessitate a large number of antenna elements and the transmission of numerous chirps, leading to increased hardware and computational complexity. While compressive sensing (CS) has recently been applied to pulsed-waveform radars with sparse measurements, its application to frequency-modulated continuous wave (FMCW) radar for target detection remains largely unexplored. In this paper, we propose a novel CS-based multi-target localization algorithm in the range, Doppler, and angular domains for MIMO-FMCW radar, where we jointly estimate targets' velocities and angles of arrival. To this end, we present a signal model for sparse-random and uniform linear arrays based on three-dimensional spectral estimation. For range estimation, we propose a discrete Fourier transform (DFT)-based focusing and orthogonal matching pursuit (OMP)-based techniques, each with distinct advantages, while two-dimensional CS is used for joint Doppler-angle estimation. Leveraging the properties of structured random matrices, we establish theoretical uniform and non-uniform recovery guarantees with high probability for the proposed framework. Our numerical experiments demonstrate that our methods achieve similar detection performance and higher resolution compared to conventional DFT and MUSIC with fewer transmitted chirps and antenna elements.
</details>
<details>
<summary><u><strong>GS-KGC: A generative subgraph-based framework for knowledge graph completion with large language models (2025)</strong></u></summary>

**Authors:** Rui Yang, Jiahao Zhu, Jianping Man, Hongze Liu, Li Fang, Yi Zhou  
**Venue & Year:** Information Fusion, 2025  
**Link:** [https://doi.org/10.1016/j.inffus.2024.102868](https://doi.org/10.1016/j.inffus.2024.102868)  
**Abstract:**  
Knowledge graph completion (KGC) focuses on identifying missing triples in a knowledge graph (KG) , which is crucial for many downstream applications. Given the rapid development of large language models (LLMs), some LLM-based methods are proposed for KGC task. However, most of them focus on prompt engineering while overlooking the fact that finer-grained subgraph information can aid LLMs in generating more accurate answers. In this paper, we propose a novel completion framework called Generative Subgraph-based KGC (GS-KGC), which utilizes subgraph information as contextual reasoning and employs a QA approach to achieve the KGC task. This framework primarily includes a subgraph partitioning algorithm designed to generate negatives and neighbors. Specifically, negatives can encourage LLMs to generate a broader range of answers, while neighbors provide additional contextual insights for LLM reasoning. Furthermore, we found that GS-KGC can discover potential triples within the KGs and new facts beyond the KGs. Experiments conducted on four common KGC datasets highlight the advantages of the proposed GS-KGC, e.g., it shows a 5.6% increase in Hits@3 compared to the LLM-based model CP-KGC on the FB15k-237N, and a 9.3% increase over the LLM-based model TECHS on the ICEWS14.

</details>
<details>
<summary><u><strong>In-Context Learning with Topological Information for LLM-Based Knowledge Graph Completion (2024)</strong></u></summary>

**Authors:** Udari Madhushani Sehwag, Kassiani Papasotiriou, Jared Vann, Sumitra Ganesh  
**Venue & Year:** ICML 2024 Workshop on Structured Probabilistic Inference & Generative Modeling  
**Link:** [https://openreview.net/forum?id=eUpH8AuVQa](https://openreview.net/forum?id=eUpH8AuVQa)  
**Abstract:**  
Knowledge graphs (KGs) are crucial for representing and reasoning over structured information, supporting a wide range of applications such as information retrieval, question answering, and decision-making. However, their effectiveness is often hindered by incompleteness, limiting their potential for real-world impact. While knowledge graph completion (KGC) has been extensively studied in the literature, recent advances in generative AI models, particularly large language models (LLMs), have introduced new opportunities for innovation. In-context learning has recently emerged as a promising approach for leveraging pretrained knowledge of LLMs across a range of natural language processing tasks and has been widely adopted in both academia and industry. However, how to utilize in-context learning for effective KGC remains relatively underexplored. We develop a novel method that incorporates topological information through in-context learning to enhance KGC performance. By integrating ontological knowledge and graph structure into the context of LLMs, our approach achieves strong performance in the transductive setting i.e., nodes in the test graph dataset are present in the training graph dataset. Furthermore, we apply our approach to KGC in the more challenging inductive setting, i.e., nodes in the training graph dataset and test graph dataset are disjoint, leveraging the ontology to infer useful information about missing nodes which serve as contextual cues for the LLM during inference. Our method demonstrates superior performance compared to baselines on the ILPC-small and ILPC-large datasets.

</details>
<details>
<summary><u><strong>Making large language models perform better in knowledge graph completion (2024)</strong></u></summary>

**Authors:** Yichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu, Wen Zhang, Hlmruajun Chen  
**Venue & Year:** Proceedings of the 32nd ACM International Conference on Multimedia, 2024  
**Link:** [https://doi.org/10.1145/3664647.3681327](https://doi.org/10.1145/3664647.3681327)  
**Abstract:**  
Large language model (LLM) based knowledge graph completion (KGC) aims to predict the missing triples in the KGs with LLMs. However, research about LLM-based KGC fails to sufficiently harness LLMs' inference proficiencies, overlooking critical structural information integral to KGs. In this paper, we explore methods to incorporate structural information into the LLMs, with the overarching goal of facilitating structure-aware reasoning. We first discuss on the existing LLM paradigms like in-context learning and instruction tuning, proposing basic structural information injection approaches. Then we propose a Knowledge Prefix Adapter (KoPA) to fulfill this stated goal. KoPA uses a structural pre-training phase to comprehend the intricate entities and relations within KGs, representing them as structural embeddings. Then KoPA communicates such cross-modal structural information understanding to the LLMs through a knowledge prefix adapter which projects the structural embeddings into the textual space and obtains virtual knowledge tokens positioned as a prefix of the input prompt. We conduct comprehensive experiments and provide incisive analysis. Our code and data are available at https://github.com/zjukg/KoPA.
</details>
<details>
<summary><u><strong>LLM-based multi-level knowledge generation for few-shot knowledge graph completion (2024)</strong></u></summary>

**Authors:** Qian Li, Zhuo Chen, Cheng Ji, Shiqi Jiang, Jianxin Li  
**Venue & Year:** Proceedings of the International Joint Conference on Artificial Intelligence, 2024  
**Link:** [https://www.ijcai.org/proceedings/2024/236](https://www.ijcai.org/proceedings/2024/236)  
**Abstract:**  
Knowledge Graphs (KGs) are pivotal in various NLP applications but often grapple with incompleteness, especially due to the long-tail problem where infrequent, unpopular relationships drastically reduce the KG completion performance. In this paper, we focus on Few-shot Knowledge Graph Completion (FKGC), a task addressing these gaps in long-tail scenarios. Amidst the rapid evolution of Large Language Models, we propose a generation-based FKGC paradigm facilitated by LLM distillation. Our MuKDC framework employs multi-level knowledge distillation for few-shot KG completion, generating supplementary knowledge to mitigate data scarcity in few-shot environments. MuKDC comprises two primary components: Multi-level Knowledge Generation, which enriches the KG at various levels, and Consistency Assessment, to ensure the coherence and reliability of the generated knowledge. Most notably, our method achieves SOTA results in both FKGC and multi-modal FKGC benchmarks, significantly advancing KG completion and enhancing the understanding and application of LLMs in structured knowledge generation and assessment.
</details>
<details>
<summary><u><strong>Assessing LLMs Suitability for Knowledge Graph Completion (2024)</strong></u></summary>

**Authors:** Vasile Ionut Remus Iga, Gheorghe Cosmin Silaghi  
**Venue & Year:** International Conference on Neural-Symbolic Learning and Reasoning, 2024  
**Link:** [https://doi.org/10.1007/978-3-031-71170-1_22](https://doi.org/10.1007/978-3-031-71170-1_22)  
**Abstract:**  
Recent work has shown the capability of Large Language Models (LLMs) to solve tasks related to Knowledge Graphs, such as Knowledge Graph Completion, even in Zero- or Few-Shot paradigms. However, they are known to hallucinate answers, or output results in a non-deterministic manner, thus leading to wrongly reasoned responses, even if they satisfy the userâ€™s demands. To highlight opportunities and challenges in knowledge graphs-related tasks, we experiment with three distinguished LLMs, namely Mixtral-8x7b-Instruct-v0.1, GPT-3.5-Turbo-0125 and GPT-4o, on Knowledge Graph Completion for static knowledge graphs, using prompts constructed following the TELeR taxonomy, in Zero- and One-Shot contexts, on a Task-Oriented Dialogue system use case. When evaluated using both strict and flexible metrics measurement manners, our results show that LLMs could be fit for such a task if prompts encapsulate sufficient information and relevant examples.
</details>
<details>
<summary><u><strong>Finetuning generative large language models with discrimination instructions for knowledge graph completion (2024)</strong></u></summary>

**Authors:** Yang Liu, Xiaobin Tian, Zequn Sun, Wei Hu  
**Venue & Year:** International Semantic Web Conference, 2024  
**Link:** [https://doi.org/10.1007/978-3-031-77844-5_11](https://doi.org/10.1007/978-3-031-77844-5_11)  
**Abstract:**  
</details>
<details>
<summary><u><strong>Enhancing text-based knowledge graph completion with zero-shot large language models: A focus on semantic enhancement (2024)</strong></u></summary>

**Authors:** Rui Yang, Jiahao Zhu, Jianping Man, Li Fang, Yi Zhou  
**Venue & Year:** Knowledge-Based Systems, 2024  
**Link:** [https://doi.org/10.1016/j.knosys.2024.112155](https://doi.org/10.1016/j.knosys.2024.112155)  
**Abstract:**  
Traditional knowledge graph (KG) completion models learn embeddings to predict missing facts. Recent works attempt to complete KGs in a text-generation manner with large language models (LLMs). However, they need to ground the output of LLMs to KG entities, which inevitably brings errors. In this paper, we present a finetuning framework, DIFT, aiming to unleash the KG completion ability of LLMs and avoid grounding errors. Given an incomplete fact, DIFT employs a lightweight model to obtain candidate entities and finetunes an LLM with discrimination instructions to select the correct one from the given candidates. To improve performance while reducing instruction data, DIFT uses a truncated sampling method to select useful facts for finetuning and injects KG embeddings into the LLM. Extensive experiments on benchmark datasets demonstrate the effectiveness of our proposed framework.
</details>
<details>
<summary><u><strong>Framing Few-Shot Knowledge Graph Completion with Large Language Models (2023)</strong></u></summary>

**Authors:** Adrian MP Brasoveanu, Lyndon Nixon, Albert Weichselbraun, Arno Scharl  
**Venue & Year:** Joint Workshop Proceedings of the 5th International Workshop on Sem4Tra and SEMANTiCS, 2023  
**Link:** [https://ceur-ws.org/Vol-3510/paper_nlp_4.pdf](https://ceur-ws.org/Vol-3510/paper_nlp_4.pdf)  
**Abstract:**  
Knowledge Graph Completion (KGC) from text involves identifying known or unknown entities (nodes) as well as relations (edges) among these entities. Recent work has started to explore the use of Large Language Models (LLMs) for entity detection and relation extraction, due to their Natural Language Understanding (NLU) capabilities. However, LLM performance varies across models and depends on the quality of the prompt engineering. We examine specific relation extraction cases and present a set of
examples collected from well-known resources in a small corpus. We provide a set of annotations and identify various issues that occur when using different LLMs for this task. As LLMs will remain a focal point of future KGC research, we conclude with suggestions for improving the KGC process.
</details>
<details>
<summary><u><strong>Iterative zero-shot LLM prompting for knowledge graph construction (2023)</strong></u></summary>

**Authors:** Salvatore Carta, Alessandro Giuliani, Leonardo Piano, Alessandro Sebastian Podda, Livio Pompianu, Sandro Gabriele Tiddia  
**Venue & Year:** arXiv preprint, 2023  
**Link:** [https://doi.org/10.48550/ARXIV.2307.01128](https://doi.org/10.48550/ARXIV.2307.01128)  
**Abstract:**  
In the current digitalization era, capturing and effectively representing knowledge is crucial in most real-world scenarios. In this context, knowledge graphs represent a potent tool for retrieving and organizing a vast amount of information in a properly interconnected and interpretable structure. However, their generation is still challenging and often requires considerable human effort and domain expertise, hampering the scalability and flexibility across different application fields. This paper proposes an innovative knowledge graph generation approach that leverages the potential of the latest generative large language models, such as GPT-3.5, that can address all the main critical issues in knowledge graph building. The approach is conveyed in a pipeline that comprises novel iterative zero-shot and external knowledge-agnostic strategies in the main stages of the generation process. Our unique manifold approach may encompass significant benefits to the scientific community. In particular, the main contribution can be summarized by: (i) an innovative strategy for iteratively prompting large language models to extract relevant components of the final graph; (ii) a zero-shot strategy for each prompt, meaning that there is no need for providing examples for "guiding" the prompt result; (iii) a scalable solution, as the adoption of LLMs avoids the need for any external resources or human expertise. To assess the effectiveness of our proposed model, we performed experiments on a dataset that covered a specific domain. We claim that our proposal is a suitable solution for scalable and versatile knowledge graph construction and may be applied to different and novel contexts.
</details>
<details>
<summary><u><strong>Exploring large language models for knowledge graph completion (2023)</strong></u></summary>

**Authors:** Liang Yao, Jiazhen Peng, Chengsheng Mao, Yuan Luo  
**Venue & Year:** IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2025), 2025  
**Link:** [https://doi.org/10.48550/arXiv.2308.13916](https://doi.org/10.48550/arXiv.2308.13916)  
**Abstract:**  
Knowledge graphs play a vital role in numerous artificial intelligence tasks, yet they frequently face the issue of incompleteness. In this study, we explore utilizing Large Language Models (LLM) for knowledge graph completion. We consider triples in knowledge graphs as text sequences and introduce an innovative framework called Knowledge Graph LLM (KG-LLM) to model these triples. Our technique employs entity and relation descriptions of a triple as prompts and utilizes the response for predictions. Experiments on various benchmark knowledge graphs demonstrate that our method attains state-of-the-art performance in tasks such as triple classification and relation prediction. We also find that fine-tuning relatively smaller models (e.g., LLaMA-7B, ChatGLM-6B) outperforms recent ChatGPT and GPT-4.
</details>
<details>
<summary><u><strong>KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion (2023)</strong></u></summary>

**Authors:** Yanbin Wei, Qiushi Huang, Yu Zhang, James Kwok  
**Venue & Year:** Findings of the Association for Computational Linguistics: EMNLP, 2023  
**Link:** [https://doi.org/10.18653/v1/2023.findings-emnlp.580](https://doi.org/10.18653/v1/2023.findings-emnlp.580)  
**Abstract:**  
Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph incompleteness and supporting downstream applications. Many models have been proposed for KGC and they can be categorized into two main classes, including triple-based and test-based approaches. Triple-based methods struggle with long-tail entities due to limited structural information and imbalanced distributions of entities. Text-based methods alleviate this issue but require costly training for language models and specific finetuning for knowledge graphs, which limits their efficiency. To alleviate the limitations in the two approaches, in this paper, we propose KICGPT, a framework that integrates a large language model (LLM) and a triple-based KGC retriever, to alleviate the long-tail problem without incurring additional training overhead. In the proposed KICGPT model, we propose an in-context learning strategy called Knowledge Prompt, which encodes structural knowledge into demonstrations to guide LLM. Empirical results on benchmark datasets demonstrate the effectiveness of the proposed KICGPT model with lighter training overhead and no finetuning.

</details>
<details>
<summary><u><strong>Knowledge graph completion models are few-shot learners: An empirical study of relation labeling in e-commerce with LLMs (2023)</strong></u></summary>

**Authors:** Jiao Chen, Luyi Ma, Xiaohan Li, Nikhil Thakurdesai, Jianpeng Xu, Jason HD Cho, Kaushiki Nag, Evren Korpeoglu, Sushant Kumar, Kannan Achan  
**Venue & Year:** arXiv preprint, 2023  
**Link:** [https://doi.org/10.48550/arXiv.2305.09858](https://doi.org/10.48550/arXiv.2305.09858)  
**Abstract:**  
Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system performance by providing structured information about entities and their relationships, such as complementary or substitutable relations between products or product types, which can be utilized in recommender systems. However, relation labeling in KGs remains a challenging task due to the dynamic nature of e-commerce domains and the associated cost of human labor. Recently, breakthroughs in Large Language Models (LLMs) have shown surprising results in numerous natural language processing tasks. In this paper, we conduct an empirical study of LLMs for relation labeling in e-commerce KGs, investigating their powerful learning capabilities in natural language and effectiveness in predicting relations between product types with limited labeled data. We evaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets, demonstrating their ability to achieve competitive performance compared to humans on relation labeling tasks using just 1 to 5 labeled examples per relation. Additionally, we experiment with different prompt engineering techniques to examine their impact on model performance. Our results show that LLMs significantly outperform existing KG completion models in relation labeling for e-commerce KGs and exhibit performance strong enough to replace human labeling.
</details>
<details>
<summary><u><strong>Generate-on-graph: Treat LLM as both agent and KG in incomplete knowledge graph question answering (2024)</strong></u></summary>

**Authors:** Yao Xu, Shizhu He, Jiabei Chen, Zihao Wang, Yangqiu Song, Hanghang Tong, Guang Liu, Kang Liu, Jun Zhao  
**Venue & Year:** EMNLP, 2024  
**Link:** [https://doi.org/10.48550/arXiv.2404.14741](https://doi.org/10.48550/arXiv.2404.14741)  
**Abstract:**  
To address the issues of insufficient knowledge and hallucination in Large Language Models (LLMs), numerous studies have explored integrating LLMs with Knowledge Graphs (KGs). However, these methods are typically evaluated on conventional Knowledge Graph Question Answering (KGQA) with complete KGs, where all factual triples required for each question are entirely covered by the given KG. In such cases, LLMs primarily act as an agent to find answer entities within the KG, rather than effectively integrating the internal knowledge of LLMs and external knowledge sources such as KGs. In fact, KGs are often incomplete to cover all the knowledge required to answer questions. To simulate these real-world scenarios and evaluate the ability of LLMs to integrate internal and external knowledge, we propose leveraging LLMs for QA under Incomplete Knowledge Graph (IKGQA), where the provided KG lacks some of the factual triples for each question, and construct corresponding datasets. To handle IKGQA, we propose a training-free method called Generate-on-Graph (GoG), which can generate new factual triples while exploring KGs. Specifically, GoG performs reasoning through a Thinking-Searching-Generating framework, which treats LLM as both Agent and KG in IKGQA. Experimental results on two datasets demonstrate that our GoG outperforms all previous methods.
</details>


---

### Datasets, Metrics, and Tasks (Incompleteness)
#### ðŸ”¹ Robust Graph Learning
| Incompleteness | Method | Datasets | Metrics | Tasks |
|---------|--------|----------|---------|-------|
| ðŸŸ¦ Node | **LLM4NG** | `Cora`, `PubMed`, `ogbn-arxiv` | ðŸŽ¯ Accuracy | ðŸ§© Node Classification |
| ðŸŸ¦ Node | **LLM-TAG** | `Cora`, `Citeseer`, `PubMed`, `Arxiv-2023` | ðŸŽ¯ Accuracy | ðŸ§© Node Classification |
| ðŸŸ¦ Node | **SPLLM** | `PeMS03`, `PeMS04`, `PeMS07` | ðŸ“‰ MAE, RMSE, MAPE | â³ Spatiotemporal Forecasting |
| ðŸŸ¥ Label | **LLMGNN** | `Cora`, `Citeseer`, `PubMed`, `ogbn-arxiv`, `ogbn-products`, `WikiCS` | ðŸŽ¯ Accuracy | ðŸ§© Node Classification |
| ðŸŸ¨ Mixed | **GraphLLM** | `Synthetic Data` | âœ… Exact Match Accuracy | ðŸ”Ž Graph Reasoning |
| ðŸŸ¨ Mixed | **PROLINK** | `FB15k-237`, `Wikidata68K`, `NELL-995` | ðŸ“Š MRR, Hits@N | ðŸ”— Knowledge Graph Completion |
| ðŸŸ¨ Mixed | **UnIMP** | `BG`, `ZO`, `PK`, `BK`, `CS`, `ST`, `PW`, `BY`, `RR`, `WM` | ðŸ“‰ RMSE, MAE | ðŸ”„ Data Imputation |

---

#### ðŸ”¹ Few-Shot Graph Learning
| Incompleteness | Method | Datasets | Metrics | Tasks |
|---------|--------|----------|---------|-------|
| ðŸŸª Structure | **LinkGPT** | `AmazonSports`, `Amazon-Clothing`, `MAG-Geology`, `MAG-Math` | ðŸ“Š MRR, Hits@N | ðŸ”— Link Prediction |
| ðŸŸª Structure | **AnomalyLLM** | `UCI Messages`, `Blogcatalog`, `T-Finance`, `T-Social` | ðŸš¨ AUC | âš ï¸ Anomaly Detection |
| ðŸŸ¨ Mixed | **LLMDGCN** | `Cora`, `Citeseer`, `PubMed`, `Religion` | ðŸŽ¯ Accuracy | ðŸ§© Node Classification |
| ðŸŸ¨ Mixed | **HeGTa** | `IM-TQA`, `WCC`, `HiTab`, `WTQ`, `TabFact` | ðŸ“Š Macro-F1, Accuracy | ðŸ“‘ Table Understanding |
| ðŸŸ¨ Mixed | **FlexKBQA** | `GrailQA`, `WebQSP`, `KQA Pro` | âœ… Exact Match, F1, Accuracy | â“ Knowledge Graph QA |
| ðŸŸ¨ Mixed | **KGQG** | `WebQuestions`, `PathQuestions` | ðŸ“ BLEU-4, ROUGE-L, Hits@N | â“ Knowledge Graph QA |

---

#### ðŸ”¹ Knowledge Graph Completion
| Incompleteness | Method | Datasets | Metrics | Tasks |
|---------|--------|----------|---------|-------|
| ðŸŸ¦ Node | **LLM-KGC** | `ILPC` | ðŸ“Š MRR, Hits@N | ðŸ”— Knowledge Graph Completion |
| ðŸŸ¦ Node | **GS-KGC** | `WN18RR`, `FB15k-237`, `FB15k-237N`, `ICEWS14`, `ICEWS05-15` | ðŸ“Š Hits@N | ðŸ”— Knowledge Graph Completion |
| ðŸŸ¦ Node | **GLTW** | `FB15k-237`, `WN18RR`, `Wikidata5M` | ðŸ“Š MRR, Hits@N | ðŸ”— Link Prediction |
| ðŸŸ¥ Label | **KGs-LLM** | `Wikipedia` | ðŸŽ¯ F1, Precision, Recall | ðŸ—ï¸ Knowledge Graph Generation |
| ðŸŸ¨ Mixed | **FSKG** | `WN18RR`, `FB15k-237` | ðŸ“Š MRR, Hits@N | ðŸ”— Knowledge Graph Completion |
| ðŸŸ¨ Mixed | **KGLLM** | `WN11`, `FB13`, `WN18RR`, `YAGO3-10` | ðŸŽ¯ Accuracy, MRR, Hits@N | ðŸ”— Link Prediction / KGC |
| ðŸŸ¨ Mixed | **KICGPT** | `FB15k-237`, `WN18RR` | ðŸ“Š MRR, Hits@N | ðŸ”— Link Prediction |
| ðŸŸ¨ Mixed | **RL-LLM** | `Electronics`, `Instacart` | ðŸŽ¯ Precision, Recall, Accuracy | ðŸ”— Knowledge Graph Completion |
| ðŸŸ¨ Mixed | **GoG** | `Synthetic Data` | ðŸ“Š Hits@N | â“ Knowledge Graph QA |
| ðŸŸ¨ Mixed | **KoPA** | `UMLS`, `CoDeX-S`, `FB15K-237N` | ðŸŽ¯ F1, Precision, Recall, Accuracy | ðŸ”— Knowledge Graph Completion |
| ðŸŸ¨ Mixed | **LLMKG** | `Templates Easy`, `Templates Hard` | ðŸ“ Strict & Flexible Metrics | ðŸ”— Knowledge Graph Completion |
| ðŸŸ¨ Mixed | **DIFT** | `WN18RR`, `FB15k-237` | ðŸ“Š MRR, Hits@N | ðŸ”— Link Prediction / KGC |
| ðŸŸ¨ Mixed | **CP-KGC** | `WN18RR`, `FB15k-237`, `UMLS` | ðŸ“Š MRR, Hits@N | ðŸ”— Knowledge Graph Completion |
| ðŸŸ¨ Mixed | **MuKDC** | `NELL`, `Wiki` | ðŸ“Š MRR, Hits@N | ðŸ”— Knowledge Graph Completion |


---

## Imbalance in Graphs

> Real-world graphs often exhibit skewed class distributions or unbalanced structural patterns, making training difficult and biased.

### Class-Imbalanced Graph Learning

<!--
| Title | Authors | Venue & Year | Link |
|-------|---------|---------------|------|
| LLM-Empowered Class Imbalanced Graph Prompt Learning for Online Drug Trafficking Detection | Tianyi Ma, Yiyue Qian, Zehong Wang, Zheyuan Zhang, Chuxu Zhang, Yanfang Ye | arXiv preprint, 2025 | [Link](https://arxiv.org/abs/2503.01900) |
| Large language model assisted fine-grained knowledge graph construction for robotic fault diagnosis | Xingming Liao, Chong Chen, Zhuowei Wang, Ying Liu, Tao Wang, Lianglun Cheng | Advanced Engineering Informatics, 2025 | [Link](https://doi.org/10.1016/j.aei.2025.103134) |
| Low-resource knowledge graph completion based on knowledge distillation driven by large language models | Wenlong Hou, Weidong Zhao, Ning Jia, Xianhui Liu | Applied Soft Computing, 2025 | [Link](https://doi.org/10.1016/j.asoc.2024.112622) |
| Empowering graph neural network-based computational drug repositioning with large language model-inferred knowledge representation | Yaowen Gu, Zidu Xu, Carl Yang | Interdisciplinary Sciences: Computational Life Sciences, 2024 | [Link](https://doi.org/10.1007/s12539-024-00654-7) |
| Cost-Effective Label-free Node Classification with LLMs | Taiyan Zhang, Renchi Yang, Mingyu Yan, Xiaochun Ye, Dongrui Fan, Yurui Lai | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2412.11983) |
| Enhancing student performance prediction on learnersourced questions with SGNN-LLM synergy | Lin Ni, Sijie Wang, Zeyu Zhang, Xiaoxuan Li, Xianda Zheng, Paul Denny, Jiamou Liu | Proceedings of the AAAI Conference on Artificial Intelligence, 2024 | [Link](https://doi.org/10.1609/aaai.v38i21.30370) |
| Depression detection in clinical interviews with LLM-empowered structural element graph | Zhuang Chen, Jiawen Deng, Jinfeng Zhou, Jincenzi Wu, Tieyun Qian, Minlie Huang | Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2024 | [Link](https://aclanthology.org/2024.naacl-long.452/) |
| Fine-grainedly Synthesize Streaming Data Based On Large Language Models With Graph Structure Understanding For Data Sparsity | Xin Zhang, Linhai Zhang, Deyu Zhou, Guoqiang Xu | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2403.06139) |
| Large Language Model-based Augmentation for Imbalanced Node Classification on Text-Attributed Graphs | Leyao Wang, Yu Wang, Bo Ni, Yuying Zhao, Tyler Derr | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2410.16882) |
| Distilling large language models for text-attributed graph learning | Bo Pan, Zheng Zhang, Yifei Zhang, Yuntong Hu, Liang Zhao | Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, 2024 | [Link](https://doi.org/10.1145/3627673.3679830) |
| KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion | Yanbin Wei, Qiushi Huang, Yu Zhang, James Kwok | Findings of the Association for Computational Linguistics: EMNLP, 2023 | [Link](https://doi.org/10.18653/v1/2023.findings-emnlp.580) |
| Label-free node classification on graphs with large language models (LLMs) | Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang | arXiv preprint, 2023 | [Link](https://arxiv.org/abs/2310.04668) |
| Augmenting low-resource text classification with graph-grounded pre-training and prompting | Zhihao Wen, Yuan Fang | Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2023 | [Link](https://doi.org/10.1145/3539618.3591641) |
| Leveraging Large Language Models for Node Generation in Few-Shot Learning on Text-Attributed Graphs | Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang Li, Xuecang Zhang | arXiv preprints, 2023 | [Link](https://arxiv.org/abs/2310.09872) |
| LKPNR: LLM and KG for Personalized News Recommendation Framework | Xie Runfeng, Cui Xiangyang, Yan Zhou, Wang Xin, Xuan Zhanwei, Zhang Kai, et al. | arXiv preprint, 2023 | [Link](https://arxiv.org/abs/2308.12028) |
-->

<details>
<summary><u><strong>LLM-Empowered Class Imbalanced Graph Prompt Learning for Online Drug Trafficking Detection (2025)</strong></u></summary>

**Authors:** Tianyi Ma, Yiyue Qian, Zehong Wang, Zheyuan Zhang, Chuxu Zhang, Yanfang Ye  
**Venue & Year:** arXiv preprint, 2025  
**Link:** [https://arxiv.org/abs/2503.01900](https://arxiv.org/abs/2503.01900)
</details>
<details>
<summary><u><strong>Large language model assisted fine-grained knowledge graph construction for robotic fault diagnosis (2025)</strong></u></summary>

**Authors:** Xingming Liao, Chong Chen, Zhuowei Wang, Ying Liu, Tao Wang, Lianglun Cheng  
**Venue & Year:** Advanced Engineering Informatics, 2025  
**Link:** [https://doi.org/10.1016/j.aei.2025.103134](https://doi.org/10.1016/j.aei.2025.103134)
</details>
<details>
<summary><u><strong>Low-resource knowledge graph completion based on knowledge distillation driven by large language models (2025)</strong></u></summary>

**Authors:** Wenlong Hou, Weidong Zhao, Ning Jia, Xianhui Liu  
**Venue & Year:** Applied Soft Computing, 2025  
**Link:** [https://doi.org/10.1016/j.asoc.2024.112622](https://doi.org/10.1016/j.asoc.2024.112622)
</details>
<details>
<summary><u><strong>Empowering graph neural network-based computational drug repositioning with large language model-inferred knowledge representation (2024)</strong></u></summary>

**Authors:** Yaowen Gu, Zidu Xu, Carl Yang  
**Venue & Year:** Interdisciplinary Sciences: Computational Life Sciences, 2024  
**Link:** [https://doi.org/10.1007/s12539-024-00654-7](https://doi.org/10.1007/s12539-024-00654-7)
</details>
<details>
<summary><u><strong>Cost-Effective Label-free Node Classification with LLMs (2024)</strong></u></summary>

**Authors:** Taiyan Zhang, Renchi Yang, Mingyu Yan, Xiaochun Ye, Dongrui Fan, Yurui Lai  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2412.11983](https://arxiv.org/abs/2412.11983)
</details>
<details>
<summary><u><strong>Enhancing student performance prediction on learnersourced questions with SGNN-LLM synergy (2024)</strong></u></summary>

**Authors:** Lin Ni, Sijie Wang, Zeyu Zhang, Xiaoxuan Li, Xianda Zheng, Paul Denny, Jiamou Liu  
**Venue & Year:** Proceedings of the AAAI Conference on Artificial Intelligence, 2024  
**Link:** [https://doi.org/10.1609/aaai.v38i21.30370](https://doi.org/10.1609/aaai.v38i21.30370)
</details>
<details>
<summary><u><strong>Depression detection in clinical interviews with LLM-empowered structural element graph (2024)</strong></u></summary>

**Authors:** Zhuang Chen, Jiawen Deng, Jinfeng Zhou, Jincenzi Wu, Tieyun Qian, Minlie Huang  
**Venue & Year:** Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2024  
**Link:** [https://aclanthology.org/2024.naacl-long.452/](https://aclanthology.org/2024.naacl-long.452/)
</details>
<details>
<summary><u><strong>Fine-grainedly Synthesize Streaming Data Based On Large Language Models With Graph Structure Understanding For Data Sparsity (2024)</strong></u></summary>

**Authors:** Xin Zhang, Linhai Zhang, Deyu Zhou, Guoqiang Xu  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2403.06139](https://arxiv.org/abs/2403.06139)
</details>
<details>
<summary><u><strong>Large Language Model-based Augmentation for Imbalanced Node Classification on Text-Attributed Graphs (2024)</strong></u></summary>

**Authors:** Leyao Wang, Yu Wang, Bo Ni, Yuying Zhao, Tyler Derr  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2410.16882](https://arxiv.org/abs/2410.16882)
</details>
<details>
<summary><u><strong>Distilling large language models for text-attributed graph learning (2024)</strong></u></summary>

**Authors:** Bo Pan, Zheng Zhang, Yifei Zhang, Yuntong Hu, Liang Zhao  
**Venue & Year:** Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, 2024  
**Link:** [https://doi.org/10.1145/3627673.3679830](https://doi.org/10.1145/3627673.3679830)
</details>
<details>
<summary><u><strong>KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion (2023)</strong></u></summary>

**Authors:** Yanbin Wei, Qiushi Huang, Yu Zhang, James Kwok  
**Venue & Year:** Findings of the Association for Computational Linguistics: EMNLP, 2023  
**Link:** [https://doi.org/10.18653/v1/2023.findings-emnlp.580](https://doi.org/10.18653/v1/2023.findings-emnlp.580)
</details>
<details>
<summary><u><strong>Label-free node classification on graphs with large language models (LLMs) (2023)</strong></u></summary>

**Authors:** Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang  
**Venue & Year:** arXiv preprint, 2023  
**Link:** [https://arxiv.org/abs/2310.04668](https://arxiv.org/abs/2310.04668)
</details>
<details>
<summary><u><strong>Augmenting low-resource text classification with graph-grounded pre-training and prompting (2023)</strong></u></summary>

**Authors:** Zhihao Wen, Yuan Fang  
**Venue & Year:** Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2023  
**Link:** [https://doi.org/10.1145/3539618.3591641](https://doi.org/10.1145/3539618.3591641)
</details>
<details>
<summary><u><strong>Leveraging Large Language Models for Node Generation in Few-Shot Learning on Text-Attributed Graphs (2023)</strong></u></summary>

**Authors:** Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang Li, Xuecang Zhang  
**Venue & Year:** arXiv preprints, 2023  
**Link:** [https://arxiv.org/abs/2310.09872](https://arxiv.org/abs/2310.09872)
</details>
<details>
<summary><u><strong>LKPNR: LLM and KG for Personalized News Recommendation Framework (2023)</strong></u></summary>

**Authors:** Xie Runfeng, Cui Xiangyang, Yan Zhou, Wang Xin, Xuan Zhanwei, Zhang Kai, et al.  
**Venue & Year:** arXiv preprint, 2023  
**Link:** [https://arxiv.org/abs/2308.12028](https://arxiv.org/abs/2308.12028)
</details>

---


###  Structure-Imbalanced Graph Learning
<!--
| Title | Authors | Venue & Year | Link |
|-------|---------|---------------|------|
| Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks? | Zhongjian Zhang, Xiao Wang, Huichi Zhou, Yue Yu, Mengmei Zhang, Cheng Yang, Chuan Shi | arXiv preprint, 2024 | [Link](https://doi.org/10.48550/arXiv.2408.08685) |
| Subgraph-Aware Training of Language Models for Knowledge Graph Completion Using Structure-Aware Contrastive Learning | Youmin Ko, Hyemin Yang, Taeuk Kim, Hyunjoon Kim | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2407.12703) |
| Multi-perspective improvement of knowledge graph completion with large language models | Derong Xu, Ziheng Zhang, Zhenxi Lin, Xian Wu, Zhihong Zhu, Tong Xu, Xiangyu Zhao, Yefeng Zheng, Enhong Chen | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2403.01972) |
| Graphedit: Large language models for graph structure learning | Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei, Liang Pang, Tat-Seng Chua, Chao Huang | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2402.15183) |
-->
<details>
<summary><u><strong>Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks? (2024)</strong></u></summary>

**Authors:** Zhongjian Zhang, Xiao Wang, Huichi Zhou, Yue Yu, Mengmei Zhang, Cheng Yang, Chuan Shi  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://doi.org/10.48550/arXiv.2408.08685](https://doi.org/10.48550/arXiv.2408.08685)
</details>
<details>
<summary><u><strong>Subgraph-Aware Training of Language Models for Knowledge Graph Completion Using Structure-Aware Contrastive Learning (2024)</strong></u></summary>

**Authors:** Youmin Ko, Hyemin Yang, Taeuk Kim, Hyunjoon Kim  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2407.12703](https://arxiv.org/abs/2407.12703)
</details>
<details>
<summary><u><strong>Multi-perspective improvement of knowledge graph completion with large language models (2024)</strong></u></summary>

**Authors:** Derong Xu, Ziheng Zhang, Zhenxi Lin, Xian Wu, Zhihong Zhu, Tong Xu, Xiangyu Zhao, Yefeng Zheng, Enhong Chen  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2403.01972](https://arxiv.org/abs/2403.01972)
</details>
<details>
<summary><u><strong>Graphedit: Large language models for graph structure learning (2024)</strong></u></summary>

**Authors:** Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei, Liang Pang, Tat-Seng Chua, Chao Huang  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2402.15183](https://arxiv.org/abs/2402.15183)
</details>

---

### Datasets, Metrics, and Tasks (Imbalance)

#### ðŸ”¹ Class Imbalance Graph Learning â€“ Node Classification
| Tasks | Method | Datasets | Metrics | Downstream |
|-------|--------|----------|---------|-------------|
| ðŸ§© Node Classification | **LLM4NG** | `Cora`, `PubMed`, `ogbn-arxiv` | ðŸŽ¯ Accuracy | ðŸª„ Few-shot Node Classification |
| ðŸ§© Node Classification | **LLM-GNN** | `Cora`, `Citeseer`, `PubMed`, `ogbn-arxiv`, `ogbn-products`, `WikiCS` | ðŸŽ¯ Accuracy | ðŸ·ï¸ Label-free Node Classification |
| ðŸ§© Node Classification | **G2P2** | `Cora`, `Amazon (Art, Industrial, Music Instruments)` | ðŸŽ¯ Accuracy, ðŸ“Š Macro-F1 | ðŸŒ± Zero-/Few-shot Low-resource Text Classification |
| ðŸ§© Node Classification | **LA-TAG** | `Cora`, `PubMed`, `Photo`, `Computer`, `Children` | ðŸŽ¯ Accuracy, ðŸ“Š Macro-F1 | ðŸŒ± Zero-/Few-shot Low-resource Text Classification |
| ðŸ§© Node Classification | **GSS-Net** | `Amazon (Magazine Subscriptions, Appliances, Gift Cards)` | ðŸŽ¯ Accuracy, âœ… Precision, ðŸ” Recall, ðŸ“Š F1, ðŸ“‰ MSE, RMSE, MAE | ðŸ›’ Sentiment on E-commerce Reviews |
| ðŸ§© Node Classification | **TAGrader** | `Cora`, `PubMed`, `ogbn-products`, `Arxiv-2023` | ðŸŽ¯ Accuracy | ðŸ§¾ Node Classification on TAGs |
| ðŸ§© Node Classification | **SEGA** | `DAIC-WOZ`, `EATD` | ðŸ“Š Macro-F1 | ðŸ’¬ Depression Detection |
| ðŸ§© Node Classification | **SocioHyperNet** | `MBTI` | ðŸŽ¯ Accuracy, ðŸš¨ AUC, ðŸ“Š Macro-F1, Micro-F1, IMP | ðŸ§  Personality Traits |
| ðŸ§© Node Classification | **Cella** | `Cora`, `Citeseer`, `PubMed`, `Wiki-CS` | ðŸŽ¯ Accuracy, ðŸ”— NMI, ðŸ“Š ARI, F1 | ðŸ·ï¸ Label-free Node Classification |
| ðŸ§© Node Classification | **LLM-TIKG** | `threat-dataset` | âœ… Precision, ðŸ” Recall, ðŸ“Š F1 | ðŸ›¡ï¸ Threat Intelligence KG Construction |
| ðŸ§© Node Classification | **ANLM-assInNNER** | `NE dataset` | âœ… Precision, ðŸ” Recall, ðŸ“Š F1 | ðŸ¤– Robotic Fault Diagnosis KG Construction |
| ðŸ§© Node Classification | **LLM-HetGDT** | `Twitter-HetDrug` | ðŸ“Š Macro-F1, âš–ï¸ GMean | ðŸ’Š Online Drug Trafficking Detection |

---

#### ðŸ”¹ Class Imbalance Graph Learning â€“ Prediction
| Tasks | Method | Datasets | Metrics | Downstream |
|-------|--------|----------|---------|-------------|
| ðŸ”® Prediction | **LLM-SBCL** | `biology`, `law`, `cardiff20102`, `sydney19351`, `sydney23146` | ðŸŽ¯ Accuracy, ðŸ“Š Binary-F1, Micro-F1, Macro-F1 | ðŸŽ“ Student Performance Prediction |
| ðŸ”® Prediction | **LKPNR** | `MIND` | ðŸš¨ AUC, ðŸ“Š MRR, nDCG | ðŸ“° Personalized News Recommendation |
| ðŸ”® Prediction | **LLM-DDA** | `BCFR-dataset` | ðŸš¨ AUC, ðŸ“Š AUPR, F1, âœ… Precision | ðŸ’Š Computational Drug Repositioning |

---

#### ðŸ”¹ Class Imbalance Graph Learning â€“ Graph Completion
| Tasks | Method | Datasets | Metrics | Downstream |
|-------|--------|----------|---------|-------------|
| ðŸ”— Graph Completion | **KICGPT** | `FB15k-237`, `WN18RR` | ðŸ“Š MRR, Hits@N | ðŸ”— Link Completion |
| ðŸ”— Graph Completion | **KGCD** | `WN18RR`, `YAGO3-10`, `WN18` | ðŸ“Š MRR, Hits@N | ðŸŒ± Low-resource KGC |

---

#### ðŸ”¹ Class Imbalance Graph Learning â€“ Foundation Model
| Tasks | Method | Datasets | Metrics | Downstream |
|-------|--------|----------|---------|-------------|
| ðŸ›ï¸ Foundation Model | **GraphCLIP** | `ogbn-arXiv`, `Arxiv-2023`, `PubMed`, `ogbn-products`, `Reddit`, `Cora`, `CiteSeer`, `Ele-Photo`, `Ele-Computers`, `Books-History`, `WikiCS`, `Instagram` | ðŸŽ¯ Accuracy | ðŸ”„ Transfer Learning on TAGs |

---

#### ðŸ”¹ Structure Imbalance Graph Learning
| Tasks | Method | Datasets | Metrics | Downstream |
|-------|--------|----------|---------|-------------|
| ðŸ§© Node Classification | **GraphEdit** | `Cora`, `Citeseer`, `PubMed` | ðŸŽ¯ Accuracy | âœï¸ Refining Graph Topologies |
| ðŸ”— Graph Completion | **SATKGC** | `WN18RR`, `FB15k-237`, `Wikidata5M` | ðŸ“Š MRR, Hits@N | ðŸ”— Knowledge Graph Completion |
| ðŸ”— Graph Completion | **MPIKGC** | `FB15k-237`, `WN18RR`, `FB13`, `WN11` | ðŸ“Š MR, MRR, Hits@N, ðŸŽ¯ Accuracy | ðŸ”— Knowledge Graph Completion |
| ðŸ”— Graph Completion | **LLM4RGNN** | `Cora`, `Citeseer`, `PubMed`, `ogbn-arxiv`, `ogbn-products` | ðŸŽ¯ Accuracy | ðŸ›¡ï¸ Improving Adversarial Robustness |

---

## Cross-Domain Heterogeneity in Graphs

> Graphs with heterogeneous node/edge types, multimodal attributes, or domain-specific patterns require specialized methods.

###  Text-Attributed Graph Learning
<!--
| Title | Authors | Venue & Year | Link |
|-------|---------|---------------|------|
| Hypergraph Foundation Model | Yifan Feng, Shiquan Liu, Xiangmin Han, Shaoyi Du, Zongze Wu, Han Hu, Yue Gao | arXiv preprint, 2025 | [Link](https://arxiv.org/abs/2503.01203) |
| UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language | Yufei He, Bryan Hooi | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2402.13630) |
| LLM-Align: Utilizing Large Language Models for Entity Alignment in Knowledge Graphs | Xuan Chen, Tong Lu, Zhichun Wang | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2412.04690) |
| Bootstrapping Heterogeneous Graph Representation Learning via Large Language Models: A Generalized Approach | Hang Gao, Chenhao Zhang, Fengge Wu, Junsuo Zhao, Changwen Zheng, Huaping Liu | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2412.08038) |
| Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning | Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, Bryan Hooi | International Conference on Learning Representations, 2024 | [Link](https://openreview.net/forum?id=RXFVcynVe1) |
| LLMRec: Large Language Models with Graph Augmentation for Recommendation | Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang | Proceedings of the 17th ACM International Conference on Web Search and Data Mining, 2024 | [Link](https://doi.org/10.1145/3616855.3635853) |
| Multimodal Fusion of EHR in Structures and Semantics: Integrating Clinical Records and Notes with Hypergraph and LLM | Hejie Cui, Xinyu Fang, Ran Xu, Xuan Kan, Joyce C. Ho, Carl Yang | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2403.08818) |
| One for All: Towards Training One Graph Model for All Classification Tasks | Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, Muhan Zhang | International Conference on Learning Representations, 2024 | [Link](https://openreview.net/forum?id=4IT2pgc9v6) |
-->
<details>
<summary><u><strong>Hypergraph Foundation Model (2025)</strong></u></summary>

**Authors:** Yifan Feng, Shiquan Liu, Xiangmin Han, Shaoyi Du, Zongze Wu, Han Hu, Yue Gao  
**Venue & Year:** arXiv preprint, 2025  
**Link:** [https://arxiv.org/abs/2503.01203](https://arxiv.org/abs/2503.01203)
</details>
<details>
<summary><u><strong>UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language (2024)</strong></u></summary>

**Authors:** Yufei He, Bryan Hooi  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2402.13630](https://arxiv.org/abs/2402.13630)
</details>
<details>
<summary><u><strong>LLM-Align: Utilizing Large Language Models for Entity Alignment in Knowledge Graphs (2024)</strong></u></summary>

**Authors:** Xuan Chen, Tong Lu, Zhichun Wang  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2412.04690](https://arxiv.org/abs/2412.04690)
</details>
<details>
<summary><u><strong>Bootstrapping Heterogeneous Graph Representation Learning via Large Language Models: A Generalized Approach (2024)</strong></u></summary>

**Authors:** Hang Gao, Chenhao Zhang, Fengge Wu, Junsuo Zhao, Changwen Zheng, Huaping Liu  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2412.08038](https://arxiv.org/abs/2412.08038)
</details>
<details>
<summary><u><strong>Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning (2024)</strong></u></summary>

**Authors:** Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, Bryan Hooi  
**Venue & Year:** International Conference on Learning Representations, 2024  
**Link:** [https://openreview.net/forum?id=RXFVcynVe1](https://openreview.net/forum?id=RXFVcynVe1)
</details>
<details>
<summary><u><strong>LLMRec: Large Language Models with Graph Augmentation for Recommendation (2024)</strong></u></summary>

**Authors:** Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang  
**Venue & Year:** Proceedings of the 17th ACM International Conference on Web Search and Data Mining, 2024  
**Link:** [https://doi.org/10.1145/3616855.3635853](https://doi.org/10.1145/3616855.3635853)
</details>
<details>
<summary><u><strong>Multimodal Fusion of EHR in Structures and Semantics: Integrating Clinical Records and Notes with Hypergraph and LLM (2024)</strong></u></summary>

**Authors:** Hejie Cui, Xinyu Fang, Ran Xu, Xuan Kan, Joyce C. Ho, Carl Yang  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2403.08818](https://arxiv.org/abs/2403.08818)
</details>
<details>
<summary><u><strong>One for All: Towards Training One Graph Model for All Classification Tasks (2024)</strong></u></summary>

**Authors:** Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, Muhan Zhang  
**Venue & Year:** International Conference on Learning Representations, 2024  
**Link:** [https://openreview.net/forum?id=4IT2pgc9v6](https://openreview.net/forum?id=4IT2pgc9v6)
</details>

---

###  Multimodal Attributed Graph Learning
<!--
| Title | Authors | Venue & Year | Link |
|-------|---------|---------------|------|
| UniGraph2: Learning a Unified Embedding Space to Bind Multimodal Graphs | Yufei He, Yuan Sui, Xiaoxin He, Yue Liu, Yifei Sun, Bryan Hooi | arXiv preprint, 2025 | [Link](https://arxiv.org/abs/2502.00806) |
| LLMRec: Large language models with graph augmentation for recommendation | Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang | Proceedings of the 17th ACM International Conference on Web Search and Data Mining, 2024 | [Link](https://doi.org/10.1145/3616855.3635853) |
| Touchup-G: Improving feature representation through graph-centric finetuning | Jing Zhu, Xiang Song, Vassilis Ioannidis, Danai Koutra, Christos Faloutsos | Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2024 | [Link](https://doi.org/10.1145/3626772.3657978) |
| GraphAdapter: Tuning vision-language models with dual knowledge graph | Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, Xinchao Wang | Advances in Neural Information Processing Systems, 2024 | [Link](https://openreview.net/forum?id=YmEDnMynuO&noteId=0rFYtJNqHc) |
| When Graph meets Multimodal: Benchmarking and Meditating on Multimodal Attributed Graphs Learning | Hao Yan, Chaozhuo Li, Jun Yin, Zhigang Yu, Weihao Han, Mingzheng Li, Zhengxin Zeng, Hao Sun, Senzhang Wang | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2410.09132) |
| Multimodal Graph Learning for Generative Tasks | Minji Yoon, Jing Yu Koh, Bryan Hooi, Russ Salakhutdinov | NeurIPS 2023 Workshop: New Frontiers in Graph Learning, 2023 | [Link](https://openreview.net/forum?id=YILik4gFBk) |
-->
<details>
<summary><u><strong>UniGraph2: Learning a Unified Embedding Space to Bind Multimodal Graphs (2025)</strong></u></summary>

**Authors:** Yufei He, Yuan Sui, Xiaoxin He, Yue Liu, Yifei Sun, Bryan Hooi  
**Venue & Year:** arXiv preprint, 2025  
**Link:** [https://arxiv.org/abs/2502.00806](https://arxiv.org/abs/2502.00806)
</details>
<details>
<summary><u><strong>LLMRec: Large language models with graph augmentation for recommendation (2024)</strong></u></summary>

**Authors:** Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang  
**Venue & Year:** Proceedings of the 17th ACM International Conference on Web Search and Data Mining, 2024  
**Link:** [https://doi.org/10.1145/3616855.3635853](https://doi.org/10.1145/3616855.3635853)
</details>
<details>
<summary><u><strong>Touchup-G: Improving feature representation through graph-centric finetuning (2024)</strong></u></summary>

**Authors:** Jing Zhu, Xiang Song, Vassilis Ioannidis, Danai Koutra, Christos Faloutsos  
**Venue & Year:** Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2024  
**Link:** [https://doi.org/10.1145/3626772.3657978](https://doi.org/10.1145/3626772.3657978)
</details>
<details>
<summary><u><strong>GraphAdapter: Tuning vision-language models with dual knowledge graph (2024)</strong></u></summary>

**Authors:** Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, Xinchao Wang  
**Venue & Year:** Advances in Neural Information Processing Systems, 2024  
**Link:** [https://openreview.net/forum?id=YmEDnMynuO&noteId=0rFYtJNqHc](https://openreview.net/forum?id=YmEDnMynuO&noteId=0rFYtJNqHc)
</details>
<details>
<summary><u><strong>When Graph meets Multimodal: Benchmarking and Meditating on Multimodal Attributed Graphs Learning (2024)</strong></u></summary>

**Authors:** Hao Yan, Chaozhuo Li, Jun Yin, Zhigang Yu, Weihao Han, Mingzheng Li, Zhengxin Zeng, Hao Sun, Senzhang Wang  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2410.09132](https://arxiv.org/abs/2410.09132)
</details>
<details>
<summary><u><strong>Multimodal Graph Learning for Generative Tasks (2023)</strong></u></summary>

**Authors:** Minji Yoon, Jing Yu Koh, Bryan Hooi, Russ Salakhutdinov  
**Venue & Year:** NeurIPS 2023 Workshop: New Frontiers in Graph Learning, 2023  
**Link:** [https://openreview.net/forum?id=YILik4gFBk](https://openreview.net/forum?id=YILik4gFBk)
</details>

---


###  Structural Heterogeneous Graph Learning

<!--
| Title | Authors | Venue & Year | Link |
|-------|---------|---------------|------|
| Beyond Graphs: Can Large Language Models Comprehend Hypergraphs? | Yifan Feng, Chengwu Yang, Xingliang Hou, Shaoyi Du, Shihui Ying, Zongze Wu, Yue Gao | International Conference on Learning Representations, 2025 | [Link](https://openreview.net/forum?id=28qOQwjuma) |
| UniGraph2: Learning a Unified Embedding Space to Bind Multimodal Graphs | Yufei He, Yuan Sui, Xiaoxin He, Yue Liu, Yifei Sun, Bryan Hooi | arXiv preprint, 2025 | [Link](https://arxiv.org/abs/2502.00806) |
| Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation | Wenbo Shang, Xuliang Zhu, Xin Huang | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2408.05456) |
| Graphadapter: Tuning vision-language models with dual knowledge graph | Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, Xinchao Wang | Advances in Neural Information Processing Systems, 2024 | [Link](https://openreview.net/forum?id=YmEDnMynuO&noteId=0rFYtJNqHc) |
| LLMRec: Large language models with graph augmentation for recommendation | Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang | Proceedings of the 17th ACM International Conference on Web Search and Data Mining, 2024 | [Link](https://doi.org/10.1145/3616855.3635853) |
| Talk like a Graph: Encoding Graphs for Large Language Models | Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi | International Conference on Learning Representations, 2024 | [Link](https://openreview.net/forum?id=IuXR1CCrSi) |
| Gita: Graph to visual and textual integration for vision-language graph reasoning | Yanbin Wei, Shuai Fu, Weisen Jiang, Zejian Zhang, Zhixiong Zeng, Qi Wu, James Kwok, Yu Zhang | Advances in Neural Information Processing Systems, 2024 | [Link](https://openreview.net/forum?id=SaodQ13jga) |
| WalkLM: A uniform language model fine-tuning framework for attributed graph embedding | Yanchao Tan, Zihao Zhou, Hang Lv, Weiming Liu, Carl Yang | Advances in Neural Information Processing Systems, 2024 | [Link](https://openreview.net/forum?id=ZrG8kTbt70) |
| Language is all a graph needs | Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang | Findings of the Association for Computational Linguistics: EACL, 2024 | [Link](https://aclanthology.org/2024.findings-eacl.132/) |
| Graph neural prompting with large language models | Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, Nitesh V Chawla, Panpan Xu | Proceedings of the AAAI Conference on Artificial Intelligence, 2024 | [Link](https://doi.org/10.1609/aaai.v38i17.29875) |
| Let your graph do the talking: Encoding structured data for LLMs | Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, Jonathan Halcrow | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2402.05862) |
| GraphGPT: Graph instruction tuning for large language models | Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Suqi Cheng, Dawei Yin, Chao Huang | Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2024 | [Link](https://doi.org/10.1145/3626772.3657775) |
| Higpt: Heterogeneous graph language model | Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, Chao Huang | Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2024 | [Link](https://doi.org/10.1145/3637528.3671987) |
| GPT4Graph: Can large language models understand graph structured data? An empirical evaluation and benchmarking | Jiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi He, Shi Han | arXiv preprint, 2023 | [Link](https://arxiv.org/abs/2305.15066) |
| Graphtext: Graph reasoning in text space | Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, Jian Tang | arXiv preprint, 2023 | [Link](https://arxiv.org/abs/2310.01089) |
| Can language models solve graph problems in natural language? | Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov | Advances in Neural Information Processing Systems, 2023 | [Link](https://openreview.net/forum?id=UDqHhbqYJV) |
| Evaluating large language models on graphs: Performance insights and comparative analysis | Chang Liu, Bo Wu | arXiv preprint, 2023 | [Link](https://arxiv.org/abs/2308.11224) |
-->
<details>
<summary><u><strong>Beyond Graphs: Can Large Language Models Comprehend Hypergraphs? (2025)</strong></u></summary>

**Authors:** Yifan Feng, Chengwu Yang, Xingliang Hou, Shaoyi Du, Shihui Ying, Zongze Wu, Yue Gao  
**Venue & Year:** International Conference on Learning Representations, 2025  
**Link:** [https://openreview.net/forum?id=28qOQwjuma](https://openreview.net/forum?id=28qOQwjuma)
</details>
<details>
<summary><u><strong>UniGraph2: Learning a Unified Embedding Space to Bind Multimodal Graphs (2025)</strong></u></summary>

**Authors:** Yufei He, Yuan Sui, Xiaoxin He, Yue Liu, Yifei Sun, Bryan Hooi  
**Venue & Year:** arXiv preprint, 2025  
**Link:** [https://arxiv.org/abs/2502.00806](https://arxiv.org/abs/2502.00806)
</details>
<details>
<summary><u><strong>Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation (2024)</strong></u></summary>

**Authors:** Wenbo Shang, Xuliang Zhu, Xin Huang  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2408.05456](https://arxiv.org/abs/2408.05456)
</details>
<details>
<summary><u><strong>Graphadapter: Tuning vision-language models with dual knowledge graph (2024)</strong></u></summary>

**Authors:** Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, Xinchao Wang  
**Venue & Year:** Advances in Neural Information Processing Systems, 2024  
**Link:** [https://openreview.net/forum?id=YmEDnMynuO&noteId=0rFYtJNqHc](https://openreview.net/forum?id=YmEDnMynuO&noteId=0rFYtJNqHc)
</details>
<details>
<summary><u><strong>LLMRec: Large language models with graph augmentation for recommendation (2024)</strong></u></summary>

**Authors:** Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang  
**Venue & Year:** Proceedings of the 17th ACM International Conference on Web Search and Data Mining, 2024  
**Link:** [https://doi.org/10.1145/3616855.3635853](https://doi.org/10.1145/3616855.3635853)
</details>
<details>
<summary><u><strong>Talk like a Graph: Encoding Graphs for Large Language Models (2024)</strong></u></summary>

**Authors:** Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi  
**Venue & Year:** International Conference on Learning Representations, 2024  
**Link:** [https://openreview.net/forum?id=IuXR1CCrSi](https://openreview.net/forum?id=IuXR1CCrSi)
</details>
<details>
<summary><u><strong>Gita: Graph to visual and textual integration for vision-language graph reasoning (2024)</strong></u></summary>

**Authors:** Yanbin Wei, Shuai Fu, Weisen Jiang, Zejian Zhang, Zhixiong Zeng, Qi Wu, James Kwok, Yu Zhang  
**Venue & Year:** Advances in Neural Information Processing Systems, 2024  
**Link:** [https://openreview.net/forum?id=SaodQ13jga](https://openreview.net/forum?id=SaodQ13jga)
</details>
<details>
<summary><u><strong>WalkLM: A uniform language model fine-tuning framework for attributed graph embedding (2024)</strong></u></summary>

**Authors:** Yanchao Tan, Zihao Zhou, Hang Lv, Weiming Liu, Carl Yang  
**Venue & Year:** Advances in Neural Information Processing Systems, 2024  
**Link:** [https://openreview.net/forum?id=ZrG8kTbt70](https://openreview.net/forum?id=ZrG8kTbt70)
</details>
<details>
<summary><u><strong>Language is all a graph needs (2024)</strong></u></summary>

**Authors:** Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang  
**Venue & Year:** Findings of the Association for Computational Linguistics: EACL, 2024  
**Link:** [https://aclanthology.org/2024.findings-eacl.132/](https://aclanthology.org/2024.findings-eacl.132/)
</details>
<details>
<summary><u><strong>Graph neural prompting with large language models (2024)</strong></u></summary>

**Authors:** Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, Nitesh V Chawla, Panpan Xu  
**Venue & Year:** Proceedings of the AAAI Conference on Artificial Intelligence, 2024  
**Link:** [https://doi.org/10.1609/aaai.v38i17.29875](https://doi.org/10.1609/aaai.v38i17.29875)
</details>
<details>
<summary><u><strong>Let your graph do the talking: Encoding structured data for LLMs (2024)</strong></u></summary>

**Authors:** Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, Jonathan Halcrow  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2402.05862](https://arxiv.org/abs/2402.05862)
</details>
<details>
<summary><u><strong>GraphGPT: Graph instruction tuning for large language models (2024)</strong></u></summary>

**Authors:** Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Suqi Cheng, Dawei Yin, Chao Huang  
**Venue & Year:** Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2024  
**Link:** [https://doi.org/10.1145/3626772.3657775](https://doi.org/10.1145/3626772.3657775)
</details>
<details>
<summary><u><strong>Higpt: Heterogeneous graph language model (2024)</strong></u></summary>

**Authors:** Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, Chao Huang  
**Venue & Year:** Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2024  
**Link:** [https://doi.org/10.1145/3637528.3671987](https://doi.org/10.1145/3637528.3671987)
</details>
<details>
<summary><u><strong>GPT4Graph: Can large language models understand graph structured data? An empirical evaluation and benchmarking (2023)</strong></u></summary>

**Authors:** Jiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi He, Shi Han  
**Venue & Year:** arXiv preprint, 2023  
**Link:** [https://arxiv.org/abs/2305.15066](https://arxiv.org/abs/2305.15066)
</details>
<details>
<summary><u><strong>Graphtext: Graph reasoning in text space (2023)</strong></u></summary>

**Authors:** Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, Jian Tang  
**Venue & Year:** arXiv preprint, 2023  
**Link:** [https://arxiv.org/abs/2310.01089](https://arxiv.org/abs/2310.01089)
</details>
<details>
<summary><u><strong>Can language models solve graph problems in natural language? (2023)</strong></u></summary>

**Authors:** Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov  
**Venue & Year:** Advances in Neural Information Processing Systems, 2023  
**Link:** [https://openreview.net/forum?id=UDqHhbqYJV](https://openreview.net/forum?id=UDqHhbqYJV)
</details>
<details>
<summary><u><strong>Evaluating large language models on graphs: Performance insights and comparative analysis (2023)</strong></u></summary>

**Authors:** Chang Liu, Bo Wu  
**Venue & Year:** arXiv preprint, 2023  
**Link:** [https://arxiv.org/abs/2308.11224](https://arxiv.org/abs/2308.11224)
</details>

---

### Datasets, Metrics, and Tasks (Cross-domain Heterogeneity)

#### ðŸ”¹ Text-Attributed Graph Learning â€“ Textual Attribute Alignment
| Tasks | Method | Datasets | Metrics | Downstream |
|-------|--------|----------|---------|-------------|
| ðŸ“ Textual Alignment | **TAPE** | `Cora`, `PubMed`, `Arxiv-2023`, `ogbn-arxiv`, `ogbn-products` | ðŸŽ¯ Accuracy | ðŸ§© Node Classification |
| ðŸ“ Textual Alignment | **LLMRec** | `MovieLens`, `Netflix` | âœ… Recall, nDCG, Precision | ðŸŽ¬ Item Recommendation |
| ðŸ“ Textual Alignment | **MINGLE** | `MIMIC-III`, `CRADLE` | ðŸŽ¯ Accuracy, ðŸš¨ AUC, ðŸ“Š AUPR, F1 | ðŸ§© Node Classification |
| ðŸ“ Textual Alignment | **GHGRL** | `IMDB`, `DBLP`, `ACM`, `Wiki-CS`, `IMDB-RIR`, `DBLP-RID` | ðŸ“Š Macro-F1, Micro-F1 | ðŸ§© Node Classification |

#### ðŸ”¹ Text-Attributed Graph Learning â€“ Graph Foundation Models
| Tasks | Method | Datasets | Metrics | Downstream |
|-------|--------|----------|---------|-------------|
| ðŸ›ï¸ GFM | **OFA** | `Cora`, `PubMed`, `ogbn-arxiv`, `Wiki-CS`, `MOLHIV`, `MOLPCBA`, `FB15K237`, `WN18RR`, `ChEMBL` | ðŸŽ¯ Accuracy, ðŸš¨ AUC, ðŸ“Š AUPR | ðŸ§© Node / ðŸ”— Link / ðŸ“¦ Graph Classification |
| ðŸ›ï¸ GFM | **UniGraph** | `Cora`, `PubMed`, `ogbn-arxiv`, `ogbn-products`, `Wiki-CS`, `FB15K237`, `WN18RR`, `MOLHIV`, `MOLPCBA` | ðŸš¨ AUC | ðŸ§© Node / ðŸ”— Link / ðŸ“¦ Graph Classification |
| ðŸ›ï¸ GFM | **BooG** | `Cora`, `PubMed`, `ogbn-arxiv`, `Wiki-CS`, `MOLHIV`, `MOLPCBA` | ðŸš¨ AUC | ðŸ§© Node / ðŸ“¦ Graph Classification |
| ðŸ›ï¸ GFM | **Hyper-FM** | `Cora-CA-Text`, `Cora-CC-Text`, `Pubmed-CA-Text`, `Pubmed-CC-Text`, `AminerText`, `Arxiv-Text`, `Movielens-Text`, `IMDB-Text`, `GoodBook-Text`, `PPI-Text` | ðŸŽ¯ Accuracy | ðŸ§© Node Classification |

---

#### ðŸ”¹ Multimodal Attributed Graph Learning â€“ MLLM-based Alignment
| Tasks | Method | Datasets | Metrics | Downstream |
|-------|--------|----------|---------|-------------|
| ðŸ–¼ï¸ MLLM Align | **LLMRec** | `MovieLens`, `Netflix` | âœ… Recall, nDCG, Precision | ðŸŽ¬ Item Recommendation |
| ðŸ–¼ï¸ MLLM Align | **MAGB** | `Cora`, `Wiki-CS`, `Ele-Photo`, `Flickr`, `Movies`, `Toys`, `Grocery`, `Reddit-S`, `Reddit-M` | ðŸŽ¯ Accuracy, ðŸ“Š F1 | ðŸ§© Node Classification |

#### ðŸ”¹ Multimodal Attributed Graph Learning â€“ Graph-Enhanced Alignment
| Tasks | Method | Datasets | Metrics | Downstream |
|-------|--------|----------|---------|-------------|
| ðŸ–¼ï¸ Graph Align | **MMGL** | `WikiWeb2M` | ðŸ“ BLEU-4, ROUGE-L, CIDEr | ðŸ“‘ Section Summarization |
| ðŸ–¼ï¸ Graph Align | **GraphAdapter** | `ImageNet`, `StanfordCars`, `UCF101`, `Caltech101`, `Flowers102`, `SUN397`, `DTD`, `EuroSAT`, `FGVCAircraft`, `OxfordPets`, `Food101` | ðŸŽ¯ Accuracy | ðŸ–¼ï¸ Image Classification |
| ðŸ–¼ï¸ Graph Align | **TouchUp-G** | `ogbn-arxiv`, `ogbn-products`, `Books`, `Amazon-CP` | ðŸ“Š MRR, Hits@N, ðŸŽ¯ Accuracy | ðŸ”— Link Prediction, ðŸ§© Node Classification |
| ðŸ–¼ï¸ Graph Align | **UniGraph2** | `Cora`, `PubMed`, `ogbn-arxiv`, `ogbn-papers100M`, `ogbn-products`, `Wiki-CS`, `FB15K237`, `WN18RR`, `Amazon-Sports`, `Amazon-Cloth`, `Goodreads-LP`, `Goodreads-NC`, `Ele-Fashion`, `WikiWeb2M` | ðŸŽ¯ Accuracy, ðŸ“ BLEU-4, ROUGE-L, CIDEr | ðŸ§© Node / ðŸ§· Edge Classification / ðŸ“‘ Section Summarization |

---

#### ðŸ”¹ Structural Heterogeneous Graph Learning â€“ Topological Graph Textualization
| Tasks | Method | Datasets | Metrics | Downstream |
|-------|--------|----------|---------|-------------|
| ðŸŒ Topo Text | **LLMtoGraph** | `Synthetic Graph Data` | ðŸŽ¯ Accuracy, ðŸ”Ž Positive Response Ratio | ðŸ§© Node Classification, ðŸ” Path Finding, ðŸ§® Pattern Matching |
| ðŸŒ Topo Text | **NLGraph** | `NLGraph` | ðŸŽ¯ Accuracy, ðŸ“ Partial Credit Score, Relative Error | ðŸ” Path Finding, ðŸ§® Pattern Matching, ðŸ”„ Topological Sort |
| ðŸŒ Topo Text | **Talk like a Graph** | `GraphQA` | ðŸŽ¯ Accuracy | ðŸ”— Link Prediction, ðŸ§® Pattern Matching |
| ðŸŒ Topo Text | **GPT4Graph** | `ogbn-arxiv`, `MOLHIV`, `MOLPCBA`, `MetaQA` | ðŸŽ¯ Accuracy | ðŸ§© Node / ðŸ“¦ Graph Classification, â“ Graph Query Language |
| ðŸŒ Topo Text | **GITA** | `GVLQA` | ðŸŽ¯ Accuracy | ðŸ”— Link Prediction, ðŸ§® Pattern Matching, ðŸ” Path Finding, ðŸ”„ Topological Sort |
| ðŸŒ Topo Text | **LLM4-Hypergraph** | `LLM4Hypergraph` | ðŸŽ¯ Accuracy | ðŸŒ€ Isomorphism Recognition, ðŸ“¦ Structure Classification, ðŸ”— Link Prediction, ðŸ” Path Finding |

---

#### ðŸ”¹ Structural Heterogeneous Graph Learning â€“ Attributed Graph Textualization
| Tasks | Method | Datasets | Metrics | Downstream |
|-------|--------|----------|---------|-------------|
| ðŸ§© Attrib Text | **GraphText** | `Cora`, `Citeseer`, `Texas`, `Wisconsin`, `Cornell` | ðŸŽ¯ Accuracy | ðŸ§© Node Classification |
| ðŸ§© Attrib Text | **WalkLM** | `PubMed`, `MIMIC-III` | ðŸ“Š Macro-F1, Micro-F1, ðŸš¨ AUC, MRR | ðŸ§© Node / ðŸ”— Link Prediction |
| ðŸ§© Attrib Text | **Path-LLM** | `Cora`, `Citeseer`, `PubMed`, `ogbn-arxiv` | ðŸ“Š Macro-F1, Micro-F1, ðŸš¨ AUC, ðŸŽ¯ Accuracy | ðŸ§© Node / ðŸ”— Link Prediction |
| ðŸ§© Attrib Text | **InstructGLM** | `Cora`, `PubMed`, `ogbn-arxiv` | ðŸŽ¯ Accuracy | ðŸ§© Node / ðŸ”— Link Prediction |
| ðŸ§© Attrib Text | **MuseGraph** | `Cora`, `ogbn-arxiv`, `MIMIC-III`, `AGENDA`, `WebNLG` | ðŸ“Š Macro-F1, Micro-F1, Weighted-F1, ðŸ“ BLEU-4, METEOR, ROUGE-L, CHRF++ | ðŸ§© Node Classification, âœï¸ Graph-to-Text Generation |
| ðŸ§© Attrib Text | **Graph-LLM** | `Cora`, `Citeseer`, `PubMed`, `ogbn-arxiv`, `ogbn-products` | ðŸŽ¯ Accuracy | ðŸ§© Node Classification |

---

#### ðŸ”¹ Structural Heterogeneous Graph Learning â€“ Graph Token Learning
| Tasks | Method | Datasets | Metrics | Downstream |
|-------|--------|----------|---------|-------------|
| ðŸ·ï¸ Token | **GNP** | `OBQA`, `ARC`, `PIQA`, `Riddle`, `PQA`, `BioASQ` | ðŸŽ¯ Accuracy | â“ Question Answering |
| ðŸ·ï¸ Token | **GraphToken** | `GraphQA` | ðŸŽ¯ Accuracy | ðŸ”— Link Prediction, ðŸ§® Pattern Matching |
| ðŸ·ï¸ Token | **GraphGPT** | `Cora`, `PubMed`, `ogbn-arxiv` | ðŸŽ¯ Accuracy, ðŸ“Š Macro-F1, ðŸš¨ AUC | ðŸ§© Node / ðŸ”— Link Prediction |
| ðŸ·ï¸ Token | **LLaGA** | `Cora`, `PubMed`, `ogbn-arxiv`, `ogbn-products` | ðŸŽ¯ Accuracy | ðŸ§© Node / ðŸ”— Link Prediction |
| ðŸ·ï¸ Token | **TEA-GLM** | `Cora`, `PubMed`, `ogbn-arxiv`, `TAG benchmark` | ðŸŽ¯ Accuracy, ðŸš¨ AUC | ðŸ§© Node / ðŸ”— Link Prediction |
| ðŸ·ï¸ Token | **HiGPT** | `IMDB`, `DBLP`, `ACM` | ðŸ“Š Macro-F1, Micro-F1, ðŸš¨ AUC | ðŸ§© Node Classification |

---

## Dynamic Instability in Graphs

> Graph structures may evolve over time or require adaptive interaction. These works explore LLMs in dynamic graph settings.

###  Querying and Reasoning
<!--
| Title | Authors | Venue & Year | Link |
|-------|---------|---------------|------|
| LLM4DyG: Can Large Language Models Solve Spatial-Temporal Problems on Dynamic Graphs? | Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Wenwu Zhu | Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2024 | [Link](https://doi.org/10.1145/3637528.3671709) |
| TimeR$^4$: Time-aware Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering | Xinying Qian, Ying Zhang, Yu Zhao, Baohang Zhou, Xuhui Sui, Li Zhang, Kehui Song | Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Miami, USA | [Link](https://aclanthology.org/2024.emnlp-main.394/) |
| Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models | Yifu Gao, Linbo Qiao, Zhigang Kan, Zhihua Wen, Yongquan He, Dongsheng Li | Findings of the Association for Computational Linguistics: ACL 2024, Bangkok, Thailand | [Link](https://aclanthology.org/2024.findings-acl.401/) |
| Unveiling LLMs: The Evolution of Latent Representations in a Dynamic Knowledge Graph | Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, Andrea Passerini | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2404.03623) |
| Large Language Models Can Learn Temporal Reasoning | Siheng Xiong, Ali Payani, Ramana Kompella, Faramarz Fekri | Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, 2024 | [Link](https://aclanthology.org/2024.acl-long.563/) |
| Chain-of-History Reasoning for Temporal Knowledge Graph Forecasting | Yuwei Xia, Ding Wang, Qiang Liu, Liang Wang, Shu Wu, Xiao-Yu Zhang | Findings of the Association for Computational Linguistics: ACL 2024, Bangkok, Thailand | [Link](https://aclanthology.org/2024.findings-acl.955/) |
| zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models | Zifeng Ding, Heling Cai, Jingpei Wu, Yunpu Ma, Ruotong Liao, Bo Xiong, Volker Tresp | Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Mexico City, Mexico | [Link](https://aclanthology.org/2024.naacl-long.104/) |
| Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning | Dong-Ho Lee, Kian Ahrabian, Woojeong Jin, Fred Morstatter, Jay Pujara | Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Singapore | [Link](https://aclanthology.org/2023.emnlp-main.36/) |
-->
## Robust Graph Learning â€“ Temporal & Dynamic Graphs

<details>
<summary><u><strong>LLM4DyG: Can Large Language Models Solve Spatial-Temporal Problems on Dynamic Graphs? (2024)</strong></u></summary>

**Authors:** Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Wenwu Zhu  
**Venue & Year:** Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2024  
**Link:** [https://doi.org/10.1145/3637528.3671709](https://doi.org/10.1145/3637528.3671709)
</details>
<details>
<summary><u><strong>TimeR$^4$: Time-aware Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering (2024)</strong></u></summary>

**Authors:** Xinying Qian, Ying Zhang, Yu Zhao, Baohang Zhou, Xuhui Sui, Li Zhang, Kehui Song  
**Venue & Year:** Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Miami, USA  
**Link:** [https://aclanthology.org/2024.emnlp-main.394/](https://aclanthology.org/2024.emnlp-main.394/)
</details>
<details>
<summary><u><strong>Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models (2024)</strong></u></summary>

**Authors:** Yifu Gao, Linbo Qiao, Zhigang Kan, Zhihua Wen, Yongquan He, Dongsheng Li  
**Venue & Year:** Findings of the Association for Computational Linguistics: ACL 2024, Bangkok, Thailand  
**Link:** [https://aclanthology.org/2024.findings-acl.401/](https://aclanthology.org/2024.findings-acl.401/)
</details>
<details>
<summary><u><strong>Unveiling LLMs: The Evolution of Latent Representations in a Dynamic Knowledge Graph (2024)</strong></u></summary>

**Authors:** Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, Andrea Passerini  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2404.03623](https://arxiv.org/abs/2404.03623)
</details>
<details>
<summary><u><strong>Large Language Models Can Learn Temporal Reasoning (2024)</strong></u></summary>

**Authors:** Siheng Xiong, Ali Payani, Ramana Kompella, Faramarz Fekri  
**Venue & Year:** Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, 2024  
**Link:** [https://aclanthology.org/2024.acl-long.563/](https://aclanthology.org/2024.acl-long.563/)
</details>
<details>
<summary><u><strong>Chain-of-History Reasoning for Temporal Knowledge Graph Forecasting (2024)</strong></u></summary>

**Authors:** Yuwei Xia, Ding Wang, Qiang Liu, Liang Wang, Shu Wu, Xiao-Yu Zhang  
**Venue & Year:** Findings of the Association for Computational Linguistics: ACL 2024, Bangkok, Thailand  
**Link:** [https://aclanthology.org/2024.findings-acl.955/](https://aclanthology.org/2024.findings-acl.955/)
</details>
<details>
<summary><u><strong>zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models (2024)</strong></u></summary>

**Authors:** Zifeng Ding, Heling Cai, Jingpei Wu, Yunpu Ma, Ruotong Liao, Bo Xiong, Volker Tresp  
**Venue & Year:** Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Mexico City, Mexico  
**Link:** [https://aclanthology.org/2024.naacl-long.104/](https://aclanthology.org/2024.naacl-long.104/)
</details>
<details>
<summary><u><strong>Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning (2023)</strong></u></summary>

**Authors:** Dong-Ho Lee, Kian Ahrabian, Woojeong Jin, Fred Morstatter, Jay Pujara  
**Venue & Year:** Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Singapore  
**Link:** [https://aclanthology.org/2023.emnlp-main.36/](https://aclanthology.org/2023.emnlp-main.36/)
</details>

---

###  Generating and Updating
<!--
| Title | Authors | Venue & Year | Link |
|-------|---------|---------------|------|
| FinDKG: Dynamic Knowledge Graphs with Large Language Models for Detecting Global Trends in Financial Markets | Xiaohui Victor Li, Francesco Sanna Passino | ACM, 2024 | [Link](https://doi.org/10.1145/3677052.3698603) |
| GenTKG: Generative Forecasting on Temporal Knowledge Graph with Large Language Models | Ruotong Liao, Xu Jia, Yangzhe Li, Yunpu Ma, Volker Tresp | Findings of the Association for Computational Linguistics: NAACL, 2024 | [Link](https://aclanthology.org/2024.findings-naacl.268/) |
| Up To Date: Automatic Updating Knowledge Graphs Using LLMs | Shahenda Hatem, Ghada Khoriba, Mohamed H. Gad-Elrab, Mohamed ElHelw | Procedia Computer Science, 2024 | [Link](https://www.sciencedirect.com/science/article/pii/S1877050924030072) |
| Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion | Wenjie Xu, Ben Liu, Miao Peng, Xu Jia, Min Peng | Findings of the Association for Computational Linguistics: ACL, 2023 | [Link](https://aclanthology.org/2023.findings-acl.493/) |
| Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning | Jiapu Wang, Kai Sun, Linhao Luo, Wei Wei, Yongli Hu, Alan Wee-Chung Liew, Shirui Pan, Baocai Yin | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2405.14170) |
| Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models | Chenhan Yuan, Qianqian Xie, Jimin Huang, Sophia Ananiadou | ACM, 2024 | [Link](https://doi.org/10.1145/3589334.3645376) |
| RealTCD: Temporal Causal Discovery from Interventional Data with Large Language Model | Peiwen Li, Xin Wang, Zeyang Zhang, Yuan Meng, Fang Shen, Yue Li, Jialong Wang, Yang Li, Wenwu Zhu | ACM, 2024 | [Link](https://doi.org/10.1145/3627673.3680042) |
| DynLLM: When Large Language Models Meet Dynamic Graph Recommendation | Ziwei Zhao, Fake Lin, Xi Zhu, Zhi Zheng, Tong Xu, Shitian Shen, Xueying Li, Zikai Yin, Enhong Chen | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2405.07580) |
-->
<details>
<summary><u><strong>FinDKG: Dynamic Knowledge Graphs with Large Language Models for Detecting Global Trends in Financial Markets (2024)</strong></u></summary>

**Authors:** Xiaohui Victor Li, Francesco Sanna Passino  
**Venue & Year:** ACM, 2024  
**Link:** [https://doi.org/10.1145/3677052.3698603](https://doi.org/10.1145/3677052.3698603)
</details>
<details>
<summary><u><strong>GenTKG: Generative Forecasting on Temporal Knowledge Graph with Large Language Models (2024)</strong></u></summary>

**Authors:** Ruotong Liao, Xu Jia, Yangzhe Li, Yunpu Ma, Volker Tresp  
**Venue & Year:** Findings of the Association for Computational Linguistics: NAACL, 2024  
**Link:** [https://aclanthology.org/2024.findings-naacl.268/](https://aclanthology.org/2024.findings-naacl.268/)
</details>
<details>
<summary><u><strong>Up To Date: Automatic Updating Knowledge Graphs Using LLMs (2024)</strong></u></summary>

**Authors:** Shahenda Hatem, Ghada Khoriba, Mohamed H. Gad-Elrab, Mohamed ElHelw  
**Venue & Year:** Procedia Computer Science, 2024  
**Link:** [https://www.sciencedirect.com/science/article/pii/S1877050924030072](https://www.sciencedirect.com/science/article/pii/S1877050924030072)
</details>
<details>
<summary><u><strong>Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion (2023)</strong></u></summary>

**Authors:** Wenjie Xu, Ben Liu, Miao Peng, Xu Jia, Min Peng  
**Venue & Year:** Findings of the Association for Computational Linguistics: ACL, 2023  
**Link:** [https://aclanthology.org/2023.findings-acl.493/](https://aclanthology.org/2023.findings-acl.493/)
</details>
<details>
<summary><u><strong>Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning (2024)</strong></u></summary>

**Authors:** Jiapu Wang, Kai Sun, Linhao Luo, Wei Wei, Yongli Hu, Alan Wee-Chung Liew, Shirui Pan, Baocai Yin  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2405.14170](https://arxiv.org/abs/2405.14170)
</details>
<details>
<summary><u><strong>Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models (2024)</strong></u></summary>

**Authors:** Chenhan Yuan, Qianqian Xie, Jimin Huang, Sophia Ananiadou  
**Venue & Year:** ACM, 2024  
**Link:** [https://doi.org/10.1145/3589334.3645376](https://doi.org/10.1145/3589334.3645376)
</details>
<details>
<summary><u><strong>RealTCD: Temporal Causal Discovery from Interventional Data with Large Language Model (2024)</strong></u></summary>

**Authors:** Peiwen Li, Xin Wang, Zeyang Zhang, Yuan Meng, Fang Shen, Yue Li, Jialong Wang, Yang Li, Wenwu Zhu  
**Venue & Year:** ACM, 2024  
**Link:** [https://doi.org/10.1145/3627673.3680042](https://doi.org/10.1145/3627673.3680042)
</details>
<details>
<summary><u><strong>DynLLM: When Large Language Models Meet Dynamic Graph Recommendation (2024)</strong></u></summary>

**Authors:** Ziwei Zhao, Fake Lin, Xi Zhu, Zhi Zheng, Tong Xu, Shitian Shen, Xueying Li, Zikai Yin, Enhong Chen  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2405.07580](https://arxiv.org/abs/2405.07580)
</details>

---

###  Evaluation and Application
<!--
| Title | Authors | Venue & Year | Link |
|-------|---------|---------------|------|
| DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph | Zhehao Zhang, Jiaao Chen, Diyi Yang | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2406.17271) |
| AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using Large Language Models | Shuo Liu, Di Yao, Lanting Fang, Zhetao Li, Wenbin Li, Kaiyu Feng, XiaoWen Ji, Jingping Bi | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2405.07626) |
| Language-Grounded Dynamic Scene Graphs for Interactive Object Search With Mobile Manipulation | Daniel Honerkamp, Martin BÃ¼chner, Fabien Despinoy, Tim Welschehold, Abhinav Valada | IEEE Robotics and Automation Letters, 2024 | [Link](https://doi.org/10.1109/LRA.2024.3441495) |
| Temporal Relational Reasoning of Large Language Models for Detecting Stock Portfolio Crashes | Kelvin J. L. Koa, Yunshan Ma, Ritchie Ng, Huanhuan Zheng, Tat-Seng Chua | arXiv preprint, 2024 | [Link](https://arxiv.org/abs/2410.17266) |
| Dynamic Benchmarking of Masked Language Models on Temporal Concept Drift with Multiple Views | Katerina Margatina, Shuai Wang, Yogarshi Vyas, Neha Anna John, Yassine Benajiba, Miguel Ballesteros | Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, 2023 | [Link](https://aclanthology.org/2023.eacl-main.211/) |
-->

## Robust Graph Learning â€“ Dynamic Evaluation & Temporal Reasoning

<details>
<summary><u><strong>DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph (2024)</strong></u></summary>

**Authors:** Zhehao Zhang, Jiaao Chen, Diyi Yang  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2406.17271](https://arxiv.org/abs/2406.17271)
</details>
<details>
<summary><u><strong>AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using Large Language Models (2024)</strong></u></summary>

**Authors:** Shuo Liu, Di Yao, Lanting Fang, Zhetao Li, Wenbin Li, Kaiyu Feng, XiaoWen Ji, Jingping Bi  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2405.07626](https://arxiv.org/abs/2405.07626)
</details>
<details>
<summary><u><strong>Language-Grounded Dynamic Scene Graphs for Interactive Object Search With Mobile Manipulation (2024)</strong></u></summary>

**Authors:** Daniel Honerkamp, Martin BÃ¼chner, Fabien Despinoy, Tim Welschehold, Abhinav Valada  
**Venue & Year:** IEEE Robotics and Automation Letters, 2024  
**Link:** [https://doi.org/10.1109/LRA.2024.3441495](https://doi.org/10.1109/LRA.2024.3441495)
</details>
<details>
<summary><u><strong>Temporal Relational Reasoning of Large Language Models for Detecting Stock Portfolio Crashes (2024)</strong></u></summary>

**Authors:** Kelvin J. L. Koa, Yunshan Ma, Ritchie Ng, Huanhuan Zheng, Tat-Seng Chua  
**Venue & Year:** arXiv preprint, 2024  
**Link:** [https://arxiv.org/abs/2410.17266](https://arxiv.org/abs/2410.17266)
</details>
<details>
<summary><u><strong>Dynamic Benchmarking of Masked Language Models on Temporal Concept Drift with Multiple Views (2023)</strong></u></summary>

**Authors:** Katerina Margatina, Shuai Wang, Yogarshi Vyas, Neha Anna John, Yassine Benajiba, Miguel Ballesteros  
**Venue & Year:** Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, 2023  
**Link:** [https://aclanthology.org/2023.eacl-main.211/](https://aclanthology.org/2023.eacl-main.211/)
</details>
---

### Datasets, Metrics, and Tasks (Dynamic Instability)

#### ðŸ”¹ Querying & Reasoning â€” Forecasting & Reasoning
| Tasks | Method | Datasets | Metrics | Downstream |
|------|--------|----------|---------|------------|
| ðŸ”® Forecasting / Reasoning | **ICL** | `WIKI`, `YAGO`, `ICEWS14`, `ICEWS18` | ðŸ“Š MRR, Hits@N | ðŸ”— Link Prediction |
| ðŸ”® Forecasting / Reasoning | **zrLLM** | `ICEWS`, `ACLED` | ðŸ“Š MRR, Hits@N | ðŸ”— Link Prediction |
| ðŸ”® Forecasting / Reasoning | **CoH** | `ICEWS14`, `ICEWS18`, `ICEWS05-15` | ðŸ“Š MRR, Hits@N | ðŸ”— Link Prediction |
| ðŸ”® Forecasting / Reasoning | **TG-LLM** | `TGQA`, `TimeQA`, `TempReason` | ðŸŽ¯ F1, Accuracy, âœ… Exact Match | â±ï¸ Temporal Reasoning |
| ðŸ”® Forecasting / Reasoning | **LLM4DyG** | `Enron`, `DBLP`, `Flights` | ðŸŽ¯ Accuracy, ðŸ“Š F1, ðŸ” Recall | ðŸ—ºï¸ Spatio-Temporal & Graph Reasoning / Querying, ðŸ”— Link Prediction |

#### ðŸ”¹ Querying & Reasoning â€” QA & Interpretability
| Tasks | Method | Datasets | Metrics | Downstream |
|------|--------|----------|---------|------------|
| â“ Temporal KGQA | **TimeR<sup>4</sup>** | `MULTITQ`, `TimeQuestions` | ðŸ“Š Hits@N | â“ Temporal KGQA |
| â“ Temporal KGQA | **GenTKGQA** | `CronQuestion`, `TimeQuestions` | ðŸ“Š Hits@N | â“ Temporal KGQA |
| ðŸ” Interpretability / Claim Verification | **Unveiling LLMs** | `FEVER`, `CLIMATE-FEVER` | âœ… Precision, ðŸ” Recall, ðŸ“Š F1, ðŸ§ª ROC AUC, ðŸŽ¯ Accuracy | ðŸ§¾ Claim Verification |

---

#### ðŸ§¬ Generating & Updating â€” Generating Structures
| Tasks | Method | Datasets | Metrics | Downstream |
|------|--------|----------|---------|------------|
| ðŸ§± Structure Generation | **FinDKG** | `WIKI`, `YAGO`, `ICEWS14` | ðŸ“Š MRR, Hits@N | ðŸ”— Link Prediction |
| ðŸ§± Structure Generation | **GenTKG** | `ICEWS14`, `ICEWS18`, `GDELT`, `YAGO` | ðŸ“Š Hits@N | ðŸ”— Link Prediction |
| ðŸ§± Structure Generation | **Up To Date** | `Wikidata` | ðŸŽ¯ Accuracy, ðŸ“ˆ Response Rate | âœ… Fact Validation, â“ QA |
| ðŸ§± Structure Generation | **PPT** | `ICEWS14`, `ICEWS18`, `ICEWS05-15` | ðŸ“Š MRR, Hits@N | ðŸ”— Link Prediction |
| ðŸ§± Structure Generation | **LLM-DA** | `ICEWS14`, `ICEWS05-15` | ðŸ“Š MRR, Hits@N | ðŸ”— Link Prediction |

#### ðŸ§¬ Generating & Updating â€” Generating Insights & Representations
| Tasks | Method | Datasets | Metrics | Downstream |
|------|--------|----------|---------|------------|
| ðŸ’¡ Insights / Explanations | **TimeLlama** | `ICEWS14`, `ICEWS18`, `ICEWS05-15` | âœ… Precision, ðŸ” Recall, ðŸ“Š F1, ðŸ“ BLEU, ROUGE | ðŸ“… Event Forecasting, ðŸ—’ï¸ Explanation Generation |
| ðŸ’¡ Causal / Anomaly | **RealTCD** | `Simulation Datasets` | ðŸ§­ SHD, SID | ðŸ§  Temporal Causal Discovery, âš ï¸ Anomaly Detection |
| ðŸ’¡ Dynamic RecSys | **DynLLM** | `Tmall`, `Alibaba` | ðŸ” Recall@K, nDCG@K | ðŸŽ¯ Dynamic Graph Recommendation / Top-K Recommendation |

---

#### ðŸ§ª Evaluation & Application â€” Model Evaluation
| Tasks | Method | Datasets | Metrics | Downstream |
|------|--------|----------|---------|------------|
| ðŸ§· Temporal Robustness / Probing | **Dynamic-TempLAMA** | `DYNAMICTEMPLAMA` | ðŸŽ¯ Accuracy, ðŸ“Š MRR, ðŸ“ ROUGE, ðŸ“Š F1 | ðŸ•’ Temporal Robustness Evaluation, ðŸ“š Factual Knowledge Probing |
| ðŸ§® Dynamic Reasoning Eval | **DARG** | `GSM8K`, `BBQ`, `BBH Navigate`, `BBH Dyck Language` | ðŸŽ¯ Accuracy, ðŸ“ CIAR, âœ… Exact Match, ðŸŽ¯ Accuracy | âž— Mathematical / ðŸ‘¥ Social / ðŸ§­ Spatial / ðŸ”£ Symbolic Reasoning |

#### ðŸ§ª Evaluation & Application â€” Downstream Applications
| Tasks | Method | Datasets | Metrics | Downstream |
|------|--------|----------|---------|------------|
| âš ï¸ Anomaly Detection | **AnomalyLLM** | `UCI Messages`, `Blogcatalog` | ðŸš¨ AUC | âš ï¸ Anomaly Detection |
| ðŸ¤– Interactive Object Search | **MoMa-LLM** | `iGibson scenes` | ðŸš¨ AUC, ðŸ” Recall | ðŸ—ºï¸ Semantic Interactive Object Search |
| ðŸ“° Event Detection (Finance) | **TRR** | `Reuters Financial News` | ðŸ§ª AUROC | ðŸ“° Event Detection |

---




## ðŸ“– Citation

If you find our work useful, please consider citing the following paper:
```bibtex
@article{li2025survey,
  title={A Survey of Large Language Models for Data Challenges in Graphs},
  author={Mengran Li, Pengyu Zhang, Wenbin Xing, Yijia Zheng, Klim Zaporojets, Junzhou Chen, Ronghui Zhang, Yong Zhang, Siyuan Gong, Jia Hu, Xiaolei Ma, Zhiyuan Liu, Paul Groth, Marcel Worring},
  journal={Expert Systems with Applications},
  pages={129643},
  year={2025},
  publisher={Elsevier}
}

@article{li2025using,
  title={Using Large Language Models to Tackle Fundamental Challenges in Graph Learning: A Comprehensive Survey},
  author={Mengran Li, Pengyu Zhang, Wenbin Xing, Yijia Zheng, Klim Zaporojets, Junzhou Chen, Ronghui Zhang, Yong Zhang, Siyuan Gong, Jia Hu, Xiaolei Ma, Zhiyuan Liu, Paul Groth, Marcel Worring},
  journal={arXiv preprint arXiv:2505.18475},
  year={2025}
}
